{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Trehalose Biomarker Data Annotations Workflow\n",
    "\n",
    "**Complete workflow for managing file annotations using the data model**\n",
    "\n",
    "## Workflow Steps:\n",
    "1. **Libraries & Configuration** - Load dependencies and configure paths\n",
    "2. **File Enumeration** - Connect to Synapse and enumerate files/folders\n",
    "3. **Annotation Management** - Create/update annotation templates dynamically\n",
    "4. **Validation** - Validate annotations against data model schemas\n",
    "5. **Application** - Apply validated annotations to Synapse entities\n",
    "\n",
    "## Key Features:\n",
    "- üîÑ **Dynamic annotation management**: Only adds new files, preserves existing annotations\n",
    "- üìã **Schema-driven validation**: Validates against ClinicalFile and OmicFile schemas\n",
    "- üéØ **Smart file type detection**: Automatically determines Clinical vs Omic data\n",
    "- üíæ **Persistent storage**: Saves annotations to `./annotations/{folder_name}.json`\n",
    "- ‚úÖ **Complete validation**: Blocks progression until all annotations are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Libraries loaded successfully\n",
      "üêç Python version: 2.3.2 (pandas)\n",
      "üîó Synapse client version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: LIBRARIES & IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import synapseclient\n",
    "from synapseclient.models import (\n",
    "    Column, ColumnType, Dataset, EntityRef, File, Folder, Project, FacetType, DatasetCollection\n",
    ")\n",
    "from typing import Dict, List, Any, Set, Union\n",
    "from synapseclient import Wiki\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìö Libraries loaded successfully\")\n",
    "print(f\"üêç Python version: {pd.__version__} (pandas)\")\n",
    "print(f\"üîó Synapse client version: {synapseclient.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  CONFIGURATION LOADED\n",
      "========================================\n",
      "üìÅ Staging Folder: syn68927891\n",
      "üèóÔ∏è  Project: syn68702804\n",
      "üìÑ Data Model Path: ../modules\n",
      "üíæ Annotations Directory: ../annotations\n",
      "üîç Dry Run Mode: False\n",
      "üìÇ Annotations directory ready: ../annotations\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: CONFIGURATION SETUP\n",
    "\n",
    "# Synapse Configuration\n",
    "STAGING_FOLDER_ID = \"syn68927891\"  # Trehalose Biomarker Data folder\n",
    "PROJECT_ID = \"syn68702804\"\n",
    "RELEASE_FOLDER_ID = \"syn68885183\"\n",
    "DATASETS_COLLECTION_ID = \"syn66496326\"\n",
    "DRY_RUN = False  # Set to False to actually apply changes\n",
    "\n",
    "# Data Model Configuration\n",
    "DATA_MODEL_PATH = \"../modules\"  # Path to YAML schema directory\n",
    "DATA_MODEL_FILE = \"../dist/ALS.yaml\"  # Main compiled data model (optional)\n",
    "\n",
    "# Annotation Management Configuration\n",
    "ANNOTATIONS_DIR = \"../annotations\"  # Local directory for annotation files\n",
    "\n",
    "# Authentication Token (replace with your token or use .synapseConfig)\n",
    "print(\"‚öôÔ∏è  CONFIGURATION LOADED\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üìÅ Staging Folder: {STAGING_FOLDER_ID}\")\n",
    "print(f\"üèóÔ∏è  Project: {PROJECT_ID}\")\n",
    "print(f\"üìÑ Data Model Path: {DATA_MODEL_PATH}\")\n",
    "print(f\"üíæ Annotations Directory: {ANNOTATIONS_DIR}\")\n",
    "print(f\"üîç Dry Run Mode: {DRY_RUN}\")\n",
    "\n",
    "# Create annotations directory if it doesn't exist\n",
    "os.makedirs(ANNOTATIONS_DIR, exist_ok=True)\n",
    "print(f\"üìÇ Annotations directory ready: {ANNOTATIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "file_enumeration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó SYNAPSE CONNECTION & FILE ENUMERATION\n",
      "=============================================\n",
      "\n",
      "UPGRADE AVAILABLE\n",
      "\n",
      "A more recent version of the Synapse Client (4.10.0) is available. Your version (4.8.0) can be upgraded by typing:\n",
      "   pip install --upgrade synapseclient\n",
      "\n",
      "Python Synapse Client version 4.10.0 release notes\n",
      "\n",
      "https://python-docs.synapse.org/news/\n",
      "\n",
      "\n",
      "Welcome, ram.ayyala!\n",
      "\n",
      "‚úÖ Connected to Synapse\n",
      "üîç Starting enumeration of folder syn68927891...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927891:Trehalose Biomarker Data]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927979:Answer Clinical Data]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927984:Somalogic]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927983:Protavio]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927980:ICON]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927982:Metabolomics]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:01<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìÅ Answer Clinical Data (syn68927979)\n",
      "   üîç Processing subfolder: Answer Clinical Data\n",
      "   üìÑ Answer Clinical Data/2024.T.15_Answer_ClinicalData_External.xlsx (syn68929229)\n",
      "   üìÅ ICON (syn68927980)\n",
      "   üîç Processing subfolder: ICON\n",
      "   üìÑ ICON/0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx (syn68929235)\n",
      "   üìÑ ICON/0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx (syn68929236)\n",
      "   üìÑ ICON/0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx (syn68929234)\n",
      "   üìÑ ICON/Total File_0869-0004-A.xlsx (syn68929233)\n",
      "   üìÅ Metabolomics (syn68927982)\n",
      "   üîç Processing subfolder: Metabolomics\n",
      "   üìÑ Metabolomics/25_0805_Trehalose_EAP_C18-neg.xlsx (syn68929241)\n",
      "   üìÑ Metabolomics/25_0805_Trehalose_EAP_C8-pos.xlsx (syn68929242)\n",
      "   üìÑ Metabolomics/25_0805_Trehalose_EAP_HILIC-neg.xlsx (syn68929239)\n",
      "   üìÑ Metabolomics/25_0805_Trehalose_EAP_HILIC-pos.xlsx (syn68929240)\n",
      "   üìÅ Protavio (syn68927983)\n",
      "   üîç Processing subfolder: Protavio\n",
      "   üìÑ Protavio/PR-147-TES_RESULTS REPORT - Extended NPX.csv (syn68929253)\n",
      "   üìÑ Protavio/PR-147-TES_STUDY REPORT_v1.0.pdf (syn68929249)\n",
      "   üìÑ Protavio/PR-147-TES_STUDY REPORT_v2.0.pdf (syn68929248)\n",
      "   üìÑ Protavio/correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx (syn68929247)\n",
      "   üìÅ Somalogic (syn68927984)\n",
      "   üîç Processing subfolder: Somalogic\n",
      "   üìÑ Somalogic/SS-25103982_SQS.pdf (syn68929255)\n",
      "   üìÑ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat (syn68929256)\n",
      "   üìÑ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat (syn68929257)\n",
      "   üìÑ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx (syn68929259)\n",
      "   üìÑ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx (syn68929258)\n",
      "üìä Found 18 files and 5 folders\n",
      "\n",
      "üìã Enumeration Complete:\n",
      "   üìÅ Folder: Trehalose Biomarker Data\n",
      "   üìä Found: 23 files/folders\n",
      "   üíæ Annotation file: ../annotations/trehalose_biomarker_data_annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Creating an ACL for entity syn72016774, which formerly inherited access control from a benefactor entity, \"ALL ALS\" (syn68702804).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: FILE ENUMERATION AND ANNOTATION CREATION\n",
    "\n",
    "def connect_to_synapse():\n",
    "    \"\"\"Connect to Synapse.\"\"\"\n",
    "    try:\n",
    "        syn = synapseclient.Synapse()\n",
    "        syn.login(authToken=SYNPASE_AUTH_TOKEN)\n",
    "        print(\"‚úÖ Connected to Synapse\")\n",
    "        return syn\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect: {e}\")\n",
    "        return None\n",
    "\n",
    "def enumerate_files_with_folders(syn, folder_id, include_folders=True, recursive=True):\n",
    "    \"\"\"Enumerate files and folders in a Synapse folder using modern API.\"\"\"\n",
    "    if not syn:\n",
    "        return {}\n",
    "    \n",
    "    items = {}\n",
    "    \n",
    "    def _process_folder(folder_obj, path_prefix=\"\"):\n",
    "        \"\"\"Process a folder object and its contents.\"\"\"\n",
    "        # Process files in this folder\n",
    "        if folder_obj.files:\n",
    "            for file in folder_obj.files:\n",
    "                try:\n",
    "                    file_entity = syn.get(file.id, downloadFile=False)\n",
    "                    file_name = file_entity.name if hasattr(file_entity, 'name') else file.name\n",
    "                    \n",
    "                    # Extract base name (remove extension)\n",
    "                    base_name = file_name\n",
    "                    for ext in ['.csv', '.txt', '.json', '.xml', '.tsv', '.xlsx', '.pdf', '.docx', '.html', '.md', '.adat']:\n",
    "                        if file_name.lower().endswith(ext.lower()):\n",
    "                            base_name = file_name[:-(len(ext))]\n",
    "                            break\n",
    "                    \n",
    "                    current_path = f\"{path_prefix}/{file_name}\" if path_prefix else file_name\n",
    "                    items[file.id] = {\n",
    "                        'name': file_name,\n",
    "                        'base_name': base_name,\n",
    "                        'id': file.id,\n",
    "                        'type': 'file',\n",
    "                        'path': current_path\n",
    "                    }\n",
    "                    print(f\"   üìÑ {current_path} ({file.id})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error getting file {file.id}: {e}\")\n",
    "        \n",
    "        # Process subfolders\n",
    "        if folder_obj.folders:\n",
    "            for subfolder in folder_obj.folders:\n",
    "                current_path = f\"{path_prefix}/{subfolder.name}\" if path_prefix else subfolder.name\n",
    "                \n",
    "                # Add folder metadata if requested\n",
    "                if include_folders:\n",
    "                    items[subfolder.id] = {\n",
    "                        'name': subfolder.name,\n",
    "                        'id': subfolder.id,\n",
    "                        'type': 'folder',\n",
    "                        'path': current_path\n",
    "                    }\n",
    "                    print(f\"   üìÅ {current_path} ({subfolder.id})\")\n",
    "                \n",
    "                # Recursively process subfolder if enabled\n",
    "                if recursive:\n",
    "                    print(f\"   üîç Processing subfolder: {current_path}\")\n",
    "                    _process_folder(subfolder, current_path)\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Starting enumeration of folder {folder_id}...\")\n",
    "        \n",
    "        # Get the folder and sync from Synapse\n",
    "        folder = Folder(id=folder_id)\n",
    "        folder = folder.sync_from_synapse(download_file=False, recursive=recursive)\n",
    "        \n",
    "        # Process the folder and all its contents\n",
    "        _process_folder(folder)\n",
    "        \n",
    "        file_count = sum(1 for item in items.values() if item['type'] == 'file')\n",
    "        folder_count = sum(1 for item in items.values() if item['type'] == 'folder')\n",
    "        \n",
    "        print(f\"üìä Found {file_count} files and {folder_count} folders\")\n",
    "        return items\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error enumerating files: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def get_staging_folder_name(syn, folder_id):\n",
    "    \"\"\"Extract folder name for annotation file naming.\"\"\"\n",
    "    try:\n",
    "        folder = syn.get(folder_id, downloadFile=False)\n",
    "        return folder.name\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting folder name: {e}\")\n",
    "        return \"unknown_folder\"\n",
    "\n",
    "def create_annotations_file_path(staging_folder_name):\n",
    "    \"\"\"Create path: ./annotations/{folder_name}_annotations.json\"\"\"\n",
    "    clean_name = staging_folder_name.lower().replace(' ', '_').replace('-', '_')\n",
    "    clean_name = re.sub(r'[^a-z0-9_]', '', clean_name)  # Remove special chars\n",
    "    return f\"{ANNOTATIONS_DIR}/{clean_name}_annotations.json\"\n",
    "\n",
    "def load_existing_annotations(file_path):\n",
    "    \"\"\"Load existing annotations if file exists.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_annotations(annotations, file_path):\n",
    "    \"\"\"Save annotations to JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(annotations, f, indent=2)\n",
    "\n",
    "# Execute file enumeration\n",
    "print(\"üîó SYNAPSE CONNECTION & FILE ENUMERATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "syn = connect_to_synapse()\n",
    "if syn:\n",
    "    files_folders = enumerate_files_with_folders(syn, STAGING_FOLDER_ID, include_folders=True, recursive=True)\n",
    "    staging_folder_name = get_staging_folder_name(syn, STAGING_FOLDER_ID)\n",
    "    annotation_file_path = create_annotations_file_path(staging_folder_name)\n",
    "    \n",
    "    print(f\"\\nüìã Enumeration Complete:\")\n",
    "    print(f\"   üìÅ Folder: {staging_folder_name}\")\n",
    "    print(f\"   üìä Found: {len(files_folders)} files/folders\")\n",
    "    print(f\"   üíæ Annotation file: {annotation_file_path}\")\n",
    "else:\n",
    "    print(\"‚ùå Could not connect to Synapse - using simulated data\")\n",
    "    files_folders = {}\n",
    "    staging_folder_name = \"trehalose_biomarker_data\"\n",
    "    annotation_file_path = create_annotations_file_path(staging_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "schema_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç LOADING DATA MODEL SCHEMAS\n",
      "==============================\n",
      "Looking for schemas in: /home/ramayyala/Documents/data-model/modules/**/*.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/base/BaseDataset.yaml\n",
      "  Found class: BaseDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/base/BaseFile.yaml\n",
      "  Found class: BaseFile\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/data-management.yaml\n",
      "  Found class: DataQuality\n",
      "  Found class: AssessmentAdministration\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/domains.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/genetic-profile.yaml\n",
      "  Found class: GeneticProfile\n",
      "  Found class: GeneticVariant\n",
      "  Found class: FamilyHistory\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/laboratory.yaml\n",
      "  Found class: LaboratoryCollection\n",
      "  Found class: LaboratoryResult\n",
      "  Found class: ChemistryPanel\n",
      "  Found class: Hematology\n",
      "  Found class: Biomarkers\n",
      "  Found class: Urinalysis\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/medical-history.yaml\n",
      "  Found class: MedicalHistoryCollection\n",
      "  Found class: MedicalHistory\n",
      "  Found class: Comorbidity\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/phenoconversion.yaml\n",
      "  Found class: Phenoconversion\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/study-management.yaml\n",
      "  Found class: StudyDisposition\n",
      "  Found class: Eligibility\n",
      "  Found class: ProtocolDeviation\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/treatments.yaml\n",
      "  Found class: Treatment\n",
      "  Found class: MedicalDevice\n",
      "  Found class: AdverseEvent\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/visits.yaml\n",
      "  Found class: Visit\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/dynamometry.yaml\n",
      "  Found class: HandHeldDynamometry\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/electrophysiology.yaml\n",
      "  Found class: ElectrophysiologyAssessment\n",
      "  Found class: NerveConductionStudy\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/neurological.yaml\n",
      "  Found class: NeurologicalAssessment\n",
      "  Found class: MotorAssessment\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/psychiatric.yaml\n",
      "  Found class: SuicideRiskAssessment\n",
      "  Found class: PsychiatricHistory\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/speech.yaml\n",
      "  Found class: DigitalSpeechAssessment\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/symptom-questionnaire.yaml\n",
      "  Found class: SymptomQuestionnaire\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/clinical/assessments/vital-signs-physical.yaml\n",
      "  Found class: VitalSigns\n",
      "  Found class: PhysicalExam\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/datasets/ClinicalDataset.yaml\n",
      "  Found class: ClinicalDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/datasets/ClinicalFile.yaml\n",
      "  Found class: ClinicalFile\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/datasets/OmicDataset.yaml\n",
      "  Found class: OmicDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/datasets/OmicFile.yaml\n",
      "  Found class: OmicFile\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/entities/AllDatasets.yaml\n",
      "  Found class: BaseDataset\n",
      "  Found class: ClinicalDataset\n",
      "  Found class: OmicDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/entities/Biospecimen.yaml\n",
      "  Found class: Biospecimen\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/entities/ClinicalAssessment.yaml\n",
      "  Found class: ClinicalAssessment\n",
      "  Found class: ALSFRSAssessment\n",
      "  Found class: CognitiveAssessment\n",
      "  Found class: StrengthAssessment\n",
      "  Found class: PulmonaryFunction\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/entities/Subject.yaml\n",
      "  Found class: Subject\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/governance/licenses.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/governance/portals.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/mixins/DatasetMixins.yaml\n",
      "  Found class: ClinicalDatasetMixin\n",
      "  Found class: OmicDatasetMixin\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/mixins/FileMixins.yaml\n",
      "  Found class: ClinicalFileMixin\n",
      "  Found class: OmicFileMixin\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/omics/assays.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/omics/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/omics/parameters.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/omics/platforms.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/portal/Dataset.yaml\n",
      "  Found class: Dataset\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/portal/File.yaml\n",
      "  Found class: File\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/reference/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/reference/file-formats.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/reference/sex.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/reference/species.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/shared/analysis-methods.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/shared/annotations.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/shared/common-enums.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/modules/shared/props.yaml\n",
      "Total classes loaded: 52\n",
      "\n",
      "‚úÖ Loaded 52 schema classes\n",
      "‚úÖ Found ClinicalFile\n",
      "‚úÖ Found OmicFile\n",
      "‚úÖ Found BaseFile\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "\n",
      "üìä Schema Analysis:\n",
      "   ClinicalFile attributes: 29\n",
      "   OmicFile attributes: 33\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: ENHANCED SCHEMA LOADING FUNCTIONS\n",
    "\n",
    "def get_all_schemas(schema_base_path=None):\n",
    "    \"\"\"Load all schema classes from YAML files in the modules directory.\"\"\"\n",
    "    if schema_base_path is None:\n",
    "        # Get the notebook's directory and find modules relative to it\n",
    "        notebook_dir = Path.cwd()\n",
    "        # Look for modules in the current directory or parent directories\n",
    "        schema_base_path = None\n",
    "        for parent in [notebook_dir] + list(notebook_dir.parents):\n",
    "            potential_path = parent / 'modules'\n",
    "            if potential_path.exists():\n",
    "                schema_base_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if schema_base_path is None:\n",
    "            # Fallback to relative path\n",
    "            schema_base_path = Path('modules')\n",
    "    \n",
    "    schema_path = str(schema_base_path / '**' / '*.yaml')\n",
    "    all_schemas = {}\n",
    "    \n",
    "    print(f\"Looking for schemas in: {schema_path}\")\n",
    "    \n",
    "    for schema_file in glob.glob(schema_path, recursive=True):\n",
    "        print(f\"Processing: {schema_file}\")\n",
    "        with open(schema_file, 'r') as f:\n",
    "            try:\n",
    "                schema = yaml.safe_load(f)\n",
    "                if schema and 'classes' in schema:\n",
    "                    for class_name, class_def in schema['classes'].items():\n",
    "                        all_schemas[class_name] = class_def\n",
    "                        print(f\"  Found class: {class_name}\")\n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"Error parsing {schema_file}: {e}\")\n",
    "    \n",
    "    print(f\"Total classes loaded: {len(all_schemas)}\")\n",
    "    return all_schemas\n",
    "\n",
    "def get_full_schema(class_name, all_schemas, visited=None):\n",
    "    \"\"\"Recursively build complete schema including inheritance and mixins.\"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    # Prevent infinite recursion\n",
    "    if class_name in visited:\n",
    "        return {}\n",
    "    visited.add(class_name)\n",
    "    \n",
    "    if class_name not in all_schemas:\n",
    "        print(f\"Warning: Class '{class_name}' not found in schemas\")\n",
    "        return {}\n",
    "    \n",
    "    class_def = all_schemas.get(class_name, {})\n",
    "    if not class_def:\n",
    "        return {}\n",
    "        \n",
    "    # Start with this class's attributes\n",
    "    attributes = class_def.get('attributes', {}).copy()\n",
    "    \n",
    "    # Add parent class attributes (is_a relationship)\n",
    "    if 'is_a' in class_def:\n",
    "        parent_name = class_def['is_a']\n",
    "        print(f\"  {class_name} inherits from {parent_name}\")\n",
    "        parent_attributes = get_full_schema(parent_name, all_schemas, visited.copy())\n",
    "        # Parent attributes come first, then are overridden by child attributes\n",
    "        attributes = {**parent_attributes, **attributes}\n",
    "        \n",
    "    # Add mixin attributes\n",
    "    if 'mixins' in class_def:\n",
    "        for mixin in class_def['mixins']:\n",
    "            print(f\"  {class_name} uses mixin {mixin}\")\n",
    "            mixin_attributes = get_full_schema(mixin, all_schemas, visited.copy())\n",
    "            # Mixins come first, then are overridden by class attributes\n",
    "            attributes = {**mixin_attributes, **attributes}\n",
    "            \n",
    "    return attributes\n",
    "\n",
    "def detect_file_type(file_info):\n",
    "    \"\"\"Detect whether a file should use ClinicalFile or OmicFile schema.\"\"\"\n",
    "    name = file_info.get('name', '').lower()\n",
    "    path = file_info.get('path', '').lower()\n",
    "    \n",
    "    # Omic data indicators\n",
    "    omic_indicators = [\n",
    "        'metabolomics', 'proteomics', 'genomics', 'transcriptomics',\n",
    "        'somalogic', 'protavio', 'sequencing', 'omics',\n",
    "        '.adat', '.fastq', '.bam', '.vcf', '.bed'\n",
    "    ]\n",
    "    \n",
    "    # Clinical data indicators  \n",
    "    clinical_indicators = [\n",
    "        'clinical', 'assessment', 'medical', 'treatment', 'visit',\n",
    "        'demographic', 'alsfrs', 'vital', 'neurological', 'answer'\n",
    "    ]\n",
    "    \n",
    "    # Check for omic indicators\n",
    "    for indicator in omic_indicators:\n",
    "        if indicator in name or indicator in path:\n",
    "            return 'OmicFile'\n",
    "    \n",
    "    # Check for clinical indicators\n",
    "    for indicator in clinical_indicators:\n",
    "        if indicator in name or indicator in path:\n",
    "            return 'ClinicalFile'\n",
    "    \n",
    "    # Default to ClinicalFile for unknown types\n",
    "    return 'ClinicalFile'\n",
    "\n",
    "# Load all schemas from the data model\n",
    "print(\"üîç LOADING DATA MODEL SCHEMAS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "all_schemas = get_all_schemas()\n",
    "\n",
    "if not all_schemas:\n",
    "    print(\"‚ùå No schemas found! Check the path to modules directory.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Loaded {len(all_schemas)} schema classes\")\n",
    "    \n",
    "    # Test schema loading for key classes\n",
    "    for class_name in ['ClinicalFile', 'OmicFile', 'BaseFile']:\n",
    "        if class_name in all_schemas:\n",
    "            print(f\"‚úÖ Found {class_name}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing {class_name}\")\n",
    "    \n",
    "    # Test schema inheritance\n",
    "    clinical_schema = get_full_schema('ClinicalFile', all_schemas)\n",
    "    omic_schema = get_full_schema('OmicFile', all_schemas)\n",
    "    \n",
    "    print(f\"\\nüìä Schema Analysis:\")\n",
    "    print(f\"   ClinicalFile attributes: {len(clinical_schema)}\")\n",
    "    print(f\"   OmicFile attributes: {len(omic_schema)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "annotation_management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  ANNOTATION STRUCTURE GENERATION\n",
      "===================================\n",
      "üìÇ Existing annotations: 23 entries\n",
      "‚úÖ Existing annotations kept for: Answer Clinical Data\n",
      "‚úÖ Existing annotations kept for: 2024.T.15_Answer_ClinicalData_External.xlsx\n",
      "‚úÖ Existing annotations kept for: ICON\n",
      "‚úÖ Existing annotations kept for: 0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx\n",
      "‚úÖ Existing annotations kept for: 0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx\n",
      "‚úÖ Existing annotations kept for: 0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx\n",
      "‚úÖ Existing annotations kept for: Total File_0869-0004-A.xlsx\n",
      "‚úÖ Existing annotations kept for: Metabolomics\n",
      "‚úÖ Existing annotations kept for: 25_0805_Trehalose_EAP_C18-neg.xlsx\n",
      "‚úÖ Existing annotations kept for: 25_0805_Trehalose_EAP_C8-pos.xlsx\n",
      "‚úÖ Existing annotations kept for: 25_0805_Trehalose_EAP_HILIC-neg.xlsx\n",
      "‚úÖ Existing annotations kept for: 25_0805_Trehalose_EAP_HILIC-pos.xlsx\n",
      "‚úÖ Existing annotations kept for: Protavio\n",
      "‚úÖ Existing annotations kept for: PR-147-TES_RESULTS REPORT - Extended NPX.csv\n",
      "‚úÖ Existing annotations kept for: PR-147-TES_STUDY REPORT_v1.0.pdf\n",
      "‚úÖ Existing annotations kept for: PR-147-TES_STUDY REPORT_v2.0.pdf\n",
      "‚úÖ Existing annotations kept for: correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx\n",
      "‚úÖ Existing annotations kept for: Somalogic\n",
      "‚úÖ Existing annotations kept for: SS-25103982_SQS.pdf\n",
      "‚úÖ Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat\n",
      "‚úÖ Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat\n",
      "‚úÖ Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx\n",
      "‚úÖ Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx\n",
      "\n",
      "üìä Merge Summary:\n",
      "   ‚ûï New entries: 0\n",
      "   ‚úÖ Existing entries: 23\n",
      "   üìã Total entries: 23\n",
      "\n",
      "üíæ Annotations saved to: ../annotations/trehalose_biomarker_data_annotations.json\n",
      "\n",
      "üìã Sample annotation structure for 'Answer Clinical Data':\n",
      "   clinicalDomain: ['']\n",
      "   studyPhase: \n",
      "   visitType: \n",
      "   analysisTypes: ['']\n",
      "   annotations: ['']\n",
      "   title: \n",
      "   description: \n",
      "   alternateName: \n",
      "   creator: ['']\n",
      "   contributor: ['']\n",
      "   ... and 22 more attributes\n",
      "\n",
      "üéØ File type detected: ClinicalFile\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: DYNAMIC ANNOTATION STRUCTURE GENERATION\n",
    "\n",
    "def create_annotation_template(file_info, all_schemas):\n",
    "    \"\"\"Create blank annotation template based on file type detection.\"\"\"\n",
    "    file_type = detect_file_type(file_info)\n",
    "    \n",
    "    # Choose appropriate schema\n",
    "    if file_type == 'OmicFile':\n",
    "        schema_attributes = get_full_schema('OmicFile', all_schemas)\n",
    "    else:\n",
    "        schema_attributes = get_full_schema('ClinicalFile', all_schemas)\n",
    "    \n",
    "    # Build template from schema\n",
    "    template = {}\n",
    "    for attr_name, attr_def in schema_attributes.items():\n",
    "        # Handle multivalued attributes\n",
    "        if isinstance(attr_def, dict) and attr_def.get('multivalued', False):\n",
    "            template[attr_name] = ['']\n",
    "        else:\n",
    "            template[attr_name] = ''\n",
    "    \n",
    "    # Add metadata about detected file type\n",
    "    template['_file_type'] = file_type\n",
    "    template['_schema_source'] = 'data-model'\n",
    "    template['_created_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    return template\n",
    "\n",
    "def merge_annotations_smartly(existing_annotations, new_files_folders, all_schemas):\n",
    "    \"\"\"\n",
    "    Smart merge that:\n",
    "    1. Keeps existing annotations intact\n",
    "    2. Adds templates for new files/folders not in existing annotations\n",
    "    3. Does not overwrite any existing values\n",
    "    \"\"\"\n",
    "    merged = existing_annotations.copy()\n",
    "    new_count = 0\n",
    "    existing_count = 0\n",
    "    \n",
    "    for syn_id, file_info in new_files_folders.items():\n",
    "        if syn_id not in merged:\n",
    "            # New file/folder - create template\n",
    "            template = create_annotation_template(file_info, all_schemas)\n",
    "            merged[syn_id] = {\n",
    "                file_info['name']: template\n",
    "            }\n",
    "            print(f\"‚ûï Added template for: {file_info['name']} (detected as {template['_file_type']})\")\n",
    "            new_count += 1\n",
    "        else:\n",
    "            print(f\"‚úÖ Existing annotations kept for: {file_info['name']}\")\n",
    "            existing_count += 1\n",
    "    \n",
    "    print(f\"\\nüìä Merge Summary:\")\n",
    "    print(f\"   ‚ûï New entries: {new_count}\")\n",
    "    print(f\"   ‚úÖ Existing entries: {existing_count}\")\n",
    "    print(f\"   üìã Total entries: {len(merged)}\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Execute annotation creation/merging\n",
    "print(\"üèóÔ∏è  ANNOTATION STRUCTURE GENERATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if all_schemas and files_folders:\n",
    "    # Load existing annotations\n",
    "    existing_annotations = load_existing_annotations(annotation_file_path)\n",
    "    print(f\"üìÇ Existing annotations: {len(existing_annotations)} entries\")\n",
    "    \n",
    "    # Smart merge with new files/folders\n",
    "    updated_annotations = merge_annotations_smartly(existing_annotations, files_folders, all_schemas)\n",
    "    \n",
    "    # Save updated annotations\n",
    "    save_annotations(updated_annotations, annotation_file_path)\n",
    "    print(f\"\\nüíæ Annotations saved to: {annotation_file_path}\")\n",
    "    \n",
    "    # Show sample annotation structure\n",
    "    if updated_annotations:\n",
    "        first_entry_id = list(updated_annotations.keys())[0]\n",
    "        first_entry = updated_annotations[first_entry_id]\n",
    "        first_file_name = list(first_entry.keys())[0]\n",
    "        sample_annotation = first_entry[first_file_name]\n",
    "        \n",
    "        print(f\"\\nüìã Sample annotation structure for '{first_file_name}':\")\n",
    "        # Show first 10 attributes\n",
    "        sample_attrs = list(sample_annotation.items())[:10]\n",
    "        for key, value in sample_attrs:\n",
    "            if isinstance(value, list) and len(value) > 3:\n",
    "                display_value = f\"{value[:3]}... (+{len(value)-3} more)\"\n",
    "            else:\n",
    "                display_value = str(value)\n",
    "            print(f\"   {key}: {display_value}\")\n",
    "        \n",
    "        if len(sample_annotation) > 10:\n",
    "            print(f\"   ... and {len(sample_annotation) - 10} more attributes\")\n",
    "        \n",
    "        print(f\"\\nüéØ File type detected: {sample_annotation.get('_file_type', 'Unknown')}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot create annotations: missing schemas or files\")\n",
    "    updated_annotations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "validation_system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ANNOTATION VALIDATION\n",
      "=========================\n",
      "üßπ Cleaning Preview:\n",
      "   üìä Total fields: 731\n",
      "   ‚úÖ Will keep: 0\n",
      "   üóëÔ∏è Will remove: 731\n",
      "   üéØ Preserved meaningful: 0\n",
      "üìÑ Answer Clinical Data...\n",
      "   Original: 29 ‚Üí Cleaned: 0 (29 removed)\n",
      "   üóëÔ∏è Removing: clinicalDomain: ['']\n",
      "üìÑ 2024.T.15_Answer_ClinicalData_External.xlsx...\n",
      "   Original: 29 ‚Üí Cleaned: 0 (29 removed)\n",
      "   üóëÔ∏è Removing: clinicalDomain: ['']\n",
      "üìÑ ICON...\n",
      "   Original: 29 ‚Üí Cleaned: 0 (29 removed)\n",
      "   üóëÔ∏è Removing: clinicalDomain: ['']\n",
      "üìã Loaded 23 annotation entries from: ../annotations/trehalose_biomarker_data_annotations.json\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå Answer Clinical Data: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 2024.T.15_Answer_ClinicalData_External.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå ICON: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå Total File_0869-0004-A.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå Metabolomics: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_C18-neg.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_C8-pos.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_HILIC-neg.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_HILIC-pos.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå Protavio: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå PR-147-TES_RESULTS REPORT - Extended NPX.csv: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå PR-147-TES_STUDY REPORT_v1.0.pdf: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå PR-147-TES_STUDY REPORT_v2.0.pdf: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå Somalogic: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_SQS.pdf: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "\n",
      "üìä Validation Summary:\n",
      "   ‚úÖ Valid: 0\n",
      "   ‚ùå Invalid: 23\n",
      "   üî¢ Total errors: 115\n",
      "   ‚ö†Ô∏è  Total warnings: 23\n",
      "\n",
      "üõë Please fix validation errors before proceeding to annotation application\n",
      "üìù Edit the annotation file: ../annotations/trehalose_biomarker_data_annotations.json\n",
      "üîÑ Re-run this cell after making changes\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: LOAD AND VALIDATE ANNOTATIONS\n",
    "\n",
    "def validate_annotation_against_schema(annotation, file_type, all_schemas):\n",
    "    \"\"\"\n",
    "    Validate individual annotation against its schema.\n",
    "    Returns (is_valid, errors_list)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Get expected schema\n",
    "    if file_type == 'OmicFile':\n",
    "        expected_schema = get_full_schema('OmicFile', all_schemas)\n",
    "    else:\n",
    "        expected_schema = get_full_schema('ClinicalFile', all_schemas)\n",
    "    \n",
    "    # Check for required fields\n",
    "    for attr_name, attr_def in expected_schema.items():\n",
    "        if isinstance(attr_def, dict) and attr_def.get('required', False):\n",
    "            if attr_name not in annotation:\n",
    "                errors.append(f\"Missing required field: {attr_name}\")\n",
    "            elif not annotation[attr_name] or annotation[attr_name] == '' or annotation[attr_name] == ['']:\n",
    "                errors.append(f\"Required field '{attr_name}' is empty\")\n",
    "    \n",
    "    # Check multivalued field constraints\n",
    "    for attr_name, value in annotation.items():\n",
    "        if attr_name.startswith('_'):  # Skip metadata fields\n",
    "            continue\n",
    "            \n",
    "        if attr_name in expected_schema:\n",
    "            attr_def = expected_schema[attr_name]\n",
    "            if isinstance(attr_def, dict):\n",
    "                is_multivalued = attr_def.get('multivalued', False)\n",
    "                \n",
    "                if is_multivalued and not isinstance(value, list):\n",
    "                    errors.append(f\"Field '{attr_name}' should be a list (multivalued)\")\n",
    "                elif not is_multivalued and isinstance(value, list):\n",
    "                    warnings.append(f\"Field '{attr_name}' is a list but should be single value\")\n",
    "        else:\n",
    "            warnings.append(f\"Field '{attr_name}' not found in schema (may be deprecated)\")\n",
    "    \n",
    "    # Check for completely empty annotations\n",
    "    non_metadata_fields = {k: v for k, v in annotation.items() if not k.startswith('_')}\n",
    "    filled_fields = {\n",
    "        k: v for k, v in non_metadata_fields.items() \n",
    "        if v and v != '' and v != [''] and v != []\n",
    "    }\n",
    "    \n",
    "    if len(filled_fields) == 0:\n",
    "        warnings.append(\"No fields have been filled out yet\")\n",
    "    \n",
    "    return len(errors) == 0, errors, warnings\n",
    "\n",
    "def validate_all_annotations(annotations_data, all_schemas):\n",
    "    \"\"\"Validate all annotations and report issues.\"\"\"\n",
    "    validation_results = {}\n",
    "    total_errors = 0\n",
    "    total_warnings = 0\n",
    "    valid_count = 0\n",
    "    \n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            file_type = annotation.get('_file_type', 'ClinicalFile')\n",
    "            is_valid, errors, warnings = validate_annotation_against_schema(annotation, file_type, all_schemas)\n",
    "            \n",
    "            validation_results[syn_id] = {\n",
    "                'file_name': file_name,\n",
    "                'is_valid': is_valid,\n",
    "                'errors': errors,\n",
    "                'warnings': warnings,\n",
    "                'file_type': file_type\n",
    "            }\n",
    "            \n",
    "            total_errors += len(errors)\n",
    "            total_warnings += len(warnings)\n",
    "            \n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                if len(warnings) > 0:\n",
    "                    print(f\"‚ö†Ô∏è  {file_name}: Valid with {len(warnings)} warnings\")\n",
    "                else:\n",
    "                    print(f\"‚úÖ {file_name}: Valid\")\n",
    "            else:\n",
    "                print(f\"‚ùå {file_name}: {len(errors)} validation errors\")\n",
    "                for error in errors[:3]:  # Show first 3 errors\n",
    "                    print(f\"   ‚Ä¢ {error}\")\n",
    "                if len(errors) > 3:\n",
    "                    print(f\"   ... and {len(errors) - 3} more errors\")\n",
    "    \n",
    "    return validation_results, total_errors, total_warnings, valid_count\n",
    "\n",
    "\n",
    "def preview_annotation_cleaning(annotations_data, sample_limit=3):\n",
    "    \"\"\"Preview what the cleaning will do to annotations.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Import the cleaning function for testing\n",
    "    def is_meaningful_value(val):\n",
    "        preserved_values = {'Unknown', 'N/A', 'unknown', 'n/a', 'NA', 'na'}\n",
    "        if val in preserved_values:\n",
    "            return True\n",
    "        if isinstance(val, str):\n",
    "            return val.strip() != ''\n",
    "        return val is not None\n",
    "    \n",
    "    stats = {\n",
    "        'total_fields': 0,\n",
    "        'cleaned_fields': 0,\n",
    "        'removed_fields': 0,\n",
    "        'preserved_meaningful': 0\n",
    "    }\n",
    "    \n",
    "    examples = []\n",
    "    count = 0\n",
    "    \n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            original_count = 0\n",
    "            cleaned_count = 0\n",
    "            preserved_examples = []\n",
    "            removed_examples = []\n",
    "            \n",
    "            for key, value in annotation.items():\n",
    "                if not key.startswith('_'):\n",
    "                    original_count += 1\n",
    "                    stats['total_fields'] += 1\n",
    "                    \n",
    "                    # Simulate cleaning logic\n",
    "                    will_keep = False\n",
    "                    if isinstance(value, list):\n",
    "                        meaningful_items = [v for v in value if is_meaningful_value(v)]\n",
    "                        if meaningful_items:\n",
    "                            will_keep = True\n",
    "                            if any(item in {'Unknown', 'N/A'} for item in meaningful_items):\n",
    "                                stats['preserved_meaningful'] += 1\n",
    "                    elif is_meaningful_value(value):\n",
    "                        will_keep = True\n",
    "                        if value in {'Unknown', 'N/A'}:\n",
    "                            stats['preserved_meaningful'] += 1\n",
    "                    \n",
    "                    if will_keep:\n",
    "                        cleaned_count += 1\n",
    "                        stats['cleaned_fields'] += 1\n",
    "                        if len(preserved_examples) < 2:\n",
    "                            preserved_examples.append(f'{key}: {str(value)[:30]}')\n",
    "                    else:\n",
    "                        stats['removed_fields'] += 1\n",
    "                        if len(removed_examples) < 2:\n",
    "                            removed_examples.append(f'{key}: {str(value)[:30]}')\n",
    "            \n",
    "            if count < sample_limit:\n",
    "                examples.append({\n",
    "                    'file_name': file_name,\n",
    "                    'original': original_count,\n",
    "                    'cleaned': cleaned_count,\n",
    "                    'removed': original_count - cleaned_count,\n",
    "                    'preserved_examples': preserved_examples,\n",
    "                    'removed_examples': removed_examples\n",
    "                })\n",
    "                count += 1\n",
    "    \n",
    "    return stats, examples\n",
    "\n",
    "# Execute validation\n",
    "print(\"üîç ANNOTATION VALIDATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "if os.path.exists(annotation_file_path):\n",
    "    # Load annotations for validation\n",
    "    annotations_data = load_existing_annotations(annotation_file_path)\n",
    "    \n",
    "    # Preview cleaning effects\n",
    "    cleaning_stats, cleaning_examples = preview_annotation_cleaning(annotations_data)\n",
    "    print(f\"üßπ Cleaning Preview:\")\n",
    "    print(f\"   üìä Total fields: {cleaning_stats['total_fields']}\")\n",
    "    print(f\"   ‚úÖ Will keep: {cleaning_stats['cleaned_fields']}\")\n",
    "    print(f\"   üóëÔ∏è Will remove: {cleaning_stats['removed_fields']}\")\n",
    "    print(f\"   üéØ Preserved meaningful: {cleaning_stats['preserved_meaningful']}\")\n",
    "    \n",
    "    for example in cleaning_examples:\n",
    "        print(f\"üìÑ {example['file_name'][:50]}...\")\n",
    "        print(f\"   Original: {example['original']} ‚Üí Cleaned: {example['cleaned']} ({example['removed']} removed)\")\n",
    "        if example['preserved_examples']:\n",
    "            print(f\"   ‚úÖ Keeping: {example['preserved_examples'][0]}\")\n",
    "        if example['removed_examples']:\n",
    "            print(f\"   üóëÔ∏è Removing: {example['removed_examples'][0]}\")\n",
    "    print(f\"üìã Loaded {len(annotations_data)} annotation entries from: {annotation_file_path}\")\n",
    "    \n",
    "    if all_schemas and annotations_data:\n",
    "        # Run validation\n",
    "        validation_results, total_errors, total_warnings, valid_count = validate_all_annotations(annotations_data, all_schemas)\n",
    "        \n",
    "        print(f\"\\nüìä Validation Summary:\")\n",
    "        print(f\"   ‚úÖ Valid: {valid_count}\")\n",
    "        print(f\"   ‚ùå Invalid: {len(validation_results) - valid_count}\")\n",
    "        print(f\"   üî¢ Total errors: {total_errors}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Total warnings: {total_warnings}\")\n",
    "        \n",
    "        if total_errors > 0:\n",
    "            print(f\"\\nüõë Please fix validation errors before proceeding to annotation application\")\n",
    "            print(f\"üìù Edit the annotation file: {annotation_file_path}\")\n",
    "            print(f\"üîÑ Re-run this cell after making changes\")\n",
    "        else:\n",
    "            print(f\"\\nüöÄ All annotations are valid! Ready to apply to Synapse entities\")\n",
    "            if total_warnings > 0:\n",
    "                print(f\"üìù Note: {total_warnings} warnings found (non-blocking)\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot validate: missing schemas or annotation data\")\n",
    "        validation_results = {}\n",
    "        total_errors = 1  # Block progression\n",
    "else:\n",
    "    print(f\"‚ùå Annotation file not found: {annotation_file_path}\")\n",
    "    print(\"üìù Please run the previous cells to create annotations first\")\n",
    "    validation_results = {}\n",
    "    total_errors = 1  # Block progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a866a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß PREPARING VARIABLES FOR APPLICATION\n",
      "===================================\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå Answer Clinical Data: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 2024.T.15_Answer_ClinicalData_External.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå ICON: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå 0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "‚ùå Total File_0869-0004-A.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå Metabolomics: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_C18-neg.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_C8-pos.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_HILIC-neg.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå 25_0805_Trehalose_EAP_HILIC-pos.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå Protavio: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå PR-147-TES_RESULTS REPORT - Extended NPX.csv: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå PR-147-TES_STUDY REPORT_v1.0.pdf: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå PR-147-TES_STUDY REPORT_v2.0.pdf: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå Somalogic: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_SQS.pdf: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "‚ùå SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx: 5 validation errors\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "‚úÖ Variables ready for application\n",
      "   syn: ‚úÖ\n",
      "   annotations_data: 23 entries\n",
      "   validation_results: 23 results\n",
      "   total_errors: 115\n"
     ]
    }
   ],
   "source": [
    "  # CELL 6.5: ENSURE VARIABLES ARE SET FOR APPLICATION\n",
    "  print(\"üîß PREPARING VARIABLES FOR APPLICATION\")\n",
    "  print(\"=\" * 35)\n",
    "\n",
    "  # Ensure all required variables exist for Cell 7\n",
    "  if 'syn' not in locals() or syn is None:\n",
    "      print(\"‚ö†Ô∏è syn variable missing - reconnecting...\")\n",
    "      syn = connect_to_synapse()\n",
    "\n",
    "  if 'annotations_data' not in locals():\n",
    "      print(\"‚ö†Ô∏è annotations_data missing - reloading...\")\n",
    "      if 'annotation_file_path' in locals() and os.path.exists(annotation_file_path):\n",
    "          annotations_data = load_existing_annotations(annotation_file_path)\n",
    "      else:\n",
    "          annotations_data = {}\n",
    "\n",
    "  if 'validation_results' not in locals():\n",
    "      print(\"‚ö†Ô∏è validation_results missing - setting empty...\")\n",
    "      validation_results = {}\n",
    "\n",
    "  if 'total_errors' not in locals():\n",
    "      print(\"‚ö†Ô∏è total_errors missing - setting to 0...\")\n",
    "      total_errors = 0\n",
    "\n",
    "  # Quick validation check\n",
    "  if annotations_data and all_schemas:\n",
    "      validation_results, total_errors, total_warnings, valid_count = validate_all_annotations(annotations_data, all_schemas)\n",
    "\n",
    "  print(f\"‚úÖ Variables ready for application\")\n",
    "  print(f\"   syn: {'‚úÖ' if syn else '‚ùå'}\")\n",
    "  print(f\"   annotations_data: {len(annotations_data) if annotations_data else 0} entries\")\n",
    "  print(f\"   validation_results: {len(validation_results)} results\")\n",
    "  print(f\"   total_errors: {total_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annotation_application",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ANNOTATION APPLICATION TO SYNAPSE\n",
      "===================================\n",
      "üîç Ready to apply annotations to 23 entities\n",
      "üìù Dry run mode: False\n",
      "üîÑ Applying annotations to Answer Clinical Data (syn68927979)\n",
      "   üìã 5 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 2024.T.15_Answer_ClinicalData_External.xlsx (syn68929229)\n",
      "   üìã 5 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to ICON (syn68927980)\n",
      "   üìã 6 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx (syn68929235)\n",
      "   üìã 6 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx (syn68929236)\n",
      "   üìã 6 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx (syn68929234)\n",
      "   üìã 6 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to Total File_0869-0004-A.xlsx (syn68929233)\n",
      "   üìã 6 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to Metabolomics (syn68927982)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 25_0805_Trehalose_EAP_C18-neg.xlsx (syn68929241)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 25_0805_Trehalose_EAP_C8-pos.xlsx (syn68929242)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 25_0805_Trehalose_EAP_HILIC-neg.xlsx (syn68929239)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to 25_0805_Trehalose_EAP_HILIC-pos.xlsx (syn68929240)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to Protavio (syn68927983)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to PR-147-TES_RESULTS REPORT - Extended NPX.csv (syn68929253)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to PR-147-TES_STUDY REPORT_v1.0.pdf (syn68929249)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to PR-147-TES_STUDY REPORT_v2.0.pdf (syn68929248)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx (syn68929247)\n",
      "   üìã 1 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to Somalogic (syn68927984)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to SS-25103982_SQS.pdf (syn68929255)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat (syn68929256)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat (syn68929257)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx (syn68929259)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "üîÑ Applying annotations to SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx (syn68929258)\n",
      "   üìã 2 non-empty fields to apply\n",
      "   ‚úÖ Success\n",
      "\n",
      "üìä Application Results:\n",
      "   ‚úÖ Success: 23\n",
      "   ‚ùå Failed: 0\n",
      "   ‚è≠Ô∏è  Skipped: 0\n",
      "   üìã Total processed: 23\n",
      "\n",
      "üéâ Successfully applied annotations to 23 entities!\n",
      "üîó Check your entities in Synapse to see the applied annotations\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: APPLY ANNOTATIONS TO SYNAPSE ENTITIES\n",
    "\n",
    "def get_existing_synapse_annotations(syn, entity_id):\n",
    "    \"\"\"Get existing annotations from Synapse entity.\"\"\"\n",
    "    try:\n",
    "        entity = syn.get(entity_id, downloadFile=False)\n",
    "        return dict(entity.annotations) if hasattr(entity, 'annotations') and entity.annotations else {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting annotations for {entity_id}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def clean_annotations_for_synapse(annotation):\n",
    "    \"\"\"Clean annotations by removing metadata fields and truly empty values.\n",
    "    \n",
    "    Preserves meaningful values like 'Unknown', 'N/A', but removes:\n",
    "    - Empty strings ('')\n",
    "    - Lists containing only empty strings (['''])\n",
    "    - Empty lists ([])\n",
    "    - None/null values\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    \n",
    "    # Values to preserve even if they might seem \"empty\"\n",
    "    preserved_values = {'Unknown', 'N/A', 'unknown', 'n/a', 'NA', 'na'}\n",
    "    \n",
    "    def is_meaningful_value(val):\n",
    "        \"\"\"Check if a value is meaningful (not truly empty).\"\"\"\n",
    "        if val in preserved_values:\n",
    "            return True\n",
    "        if isinstance(val, str):\n",
    "            return val.strip() != ''\n",
    "        return val is not None\n",
    "    \n",
    "    for key, value in annotation.items():\n",
    "        # Skip metadata fields (starting with underscore)\n",
    "        if key.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        # Handle list values\n",
    "        if isinstance(value, list):\n",
    "            # Keep only meaningful values in the list\n",
    "            cleaned_list = [v for v in value if is_meaningful_value(v)]\n",
    "            if cleaned_list:  # Only include non-empty lists\n",
    "                cleaned[key] = cleaned_list\n",
    "            # Skip completely empty lists\n",
    "        \n",
    "        # Handle single values\n",
    "        elif is_meaningful_value(value):\n",
    "            cleaned[key] = value\n",
    "        \n",
    "        # Skip truly empty values (None, '', etc.)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def apply_annotations_to_entity(syn, entity_id, new_annotations, dry_run=False):\n",
    "    \"\"\"Apply annotations to Synapse entity.\"\"\"\n",
    "    try:\n",
    "        if dry_run:\n",
    "            print(f\"üîç DRY RUN: Would apply {len(new_annotations)} annotations to {entity_id}\")\n",
    "            return True\n",
    "        \n",
    "        entity = syn.get(entity_id, downloadFile=False)\n",
    "        \n",
    "        # Clean annotations (remove metadata fields and empty values)\n",
    "        clean_annotations = clean_annotations_for_synapse(new_annotations)\n",
    "        \n",
    "        if not clean_annotations:\n",
    "            print(f\"‚ö†Ô∏è  No valid annotations to apply (all fields empty)\")\n",
    "            return True  # Not an error, just nothing to do\n",
    "        \n",
    "        entity.annotations = clean_annotations\n",
    "        syn.store(entity, forceVersion=False)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to apply annotations to {entity_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "def apply_all_annotations(syn, annotations_data, validation_results, dry_run=False):\n",
    "    \"\"\"Apply annotations to all validated entities.\"\"\"\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            # Only apply if validation passed\n",
    "            validation_result = validation_results.get(syn_id, {})\n",
    "            if not validation_result.get('is_valid', False):\n",
    "                print(f\"‚è≠Ô∏è  Skipping {file_name} (validation failed)\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"üîÑ Applying annotations to {file_name} ({syn_id})\")\n",
    "            \n",
    "            # Show what will be applied\n",
    "            clean_annotations = clean_annotations_for_synapse(annotation)\n",
    "            print(f\"   üìã {len(clean_annotations)} non-empty fields to apply\")\n",
    "            \n",
    "            success = apply_annotations_to_entity(syn, syn_id, annotation, dry_run)\n",
    "            \n",
    "            if success:\n",
    "                success_count += 1\n",
    "                print(f\"   ‚úÖ Success\")\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"   ‚ùå Failed\")\n",
    "    \n",
    "    return success_count, failed_count, skipped_count\n",
    "\n",
    "# Execute annotation application (only if validation passed)\n",
    "print(\"üöÄ ANNOTATION APPLICATION TO SYNAPSE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Re-establish required variables in this cell's scope\n",
    "try:\n",
    "    # Re-connect to Synapse if needed\n",
    "    if 'syn' not in globals() or syn is None:\n",
    "        syn = connect_to_synapse()\n",
    "    \n",
    "    # Re-load annotations if needed\n",
    "    if 'annotations_data' not in globals() or not annotations_data:\n",
    "        if 'annotation_file_path' in globals() and os.path.exists(annotation_file_path):\n",
    "            annotations_data = load_existing_annotations(annotation_file_path)\n",
    "        else:\n",
    "            annotations_data = {}\n",
    "    \n",
    "    # Re-run validation if needed\n",
    "    if 'validation_results' not in globals() or not validation_results:\n",
    "        if annotations_data and 'all_schemas' in globals() and all_schemas:\n",
    "            validation_results, total_errors, total_warnings, valid_count = validate_all_annotations(annotations_data, all_schemas)\n",
    "        else:\n",
    "            validation_results = {}\n",
    "            total_errors = 1\n",
    "    \n",
    "    # Ensure total_errors exists\n",
    "    if 'total_errors' not in globals():\n",
    "        total_errors = 0\n",
    "        \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing dependencies: {e}\")\n",
    "    print(\"üìù Please run all previous cells first\")\n",
    "    syn = None\n",
    "    annotations_data = {}\n",
    "    validation_results = {}\n",
    "    total_errors = 1\n",
    "\n",
    "# Check if we have the required data\n",
    "if not syn:\n",
    "    print(\"‚ùå No Synapse connection available\")\n",
    "    print(\"üìù Please run Cell 3 (file enumeration) first\")\n",
    "elif not annotations_data:\n",
    "    print(\"‚ùå No annotation data available\") \n",
    "    print(\"üìù Please run Cell 5 (annotation management) first\")\n",
    "elif total_errors > 0:\n",
    "    print(f\"üõë Skipping annotation application due to {total_errors} validation errors\")\n",
    "    print(f\"üìù Please fix errors and re-run Cell 6 (validation)\")\n",
    "else:\n",
    "    print(f\"üîç Ready to apply annotations to {len(annotations_data)} entities\")\n",
    "    print(f\"üìù Dry run mode: {DRY_RUN}\")\n",
    "    \n",
    "    # Apply annotations\n",
    "    success_count, failed_count, skipped_count = apply_all_annotations(\n",
    "        syn, annotations_data, validation_results, dry_run=DRY_RUN\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Application Results:\")\n",
    "    print(f\"   ‚úÖ Success: {success_count}\")\n",
    "    print(f\"   ‚ùå Failed: {failed_count}\")\n",
    "    print(f\"   ‚è≠Ô∏è  Skipped: {skipped_count}\")\n",
    "    print(f\"   üìã Total processed: {success_count + failed_count + skipped_count}\")\n",
    "    \n",
    "    if DRY_RUN:\n",
    "        print(f\"\\nüîç This was a DRY RUN - no actual changes made\")\n",
    "        print(f\"üí° Set DRY_RUN = False in Cell 2 to apply changes\")\n",
    "    elif success_count > 0:\n",
    "        print(f\"\\nüéâ Successfully applied annotations to {success_count} entities!\")\n",
    "        print(f\"üîó Check your entities in Synapse to see the applied annotations\")\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {failed_count} entities failed to update - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15un758bdsl",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/entity/syn71975076/table/transaction/async:   0%|          | 0.00/1.00 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä MINIMAL ENTITY VIEW CREATION\n",
      "===================================\n",
      "üîç Creating minimal Entity View: Trehalose_Biomarker_Data_Entity_View\n",
      "   üìÅ Scope: syn68927891\n",
      "   üèóÔ∏è Project: syn68702804\n",
      "‚ö†Ô∏è Found existing Entity View: Trehalose_Biomarker_Data_Entity_View (syn71975076)\n",
      "   üóëÔ∏è Deleting existing view to recreate...\n",
      "üîç Extracting minimal annotation columns with data...\n",
      "   üìã Found 3 essential columns with data\n",
      "   ‚ûï clinicalDomain: STRING_LIST\n",
      "   ‚ûï studyPhase: STRING\n",
      "   ‚ûï dataType: STRING_LIST\n",
      "‚úÖ Created 3 essential annotation columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Column 'name' already exists in dataset. Overwriting with default column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_name (Add)]: Column(id='81722', name='name', column_type=STRING, facet_type=None, default_value=None, maximum_size=256, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_clinicalDomain (Add)]: Column(id=None, name='clinicalDomain', column_type=STRING_LIST, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_studyPhase (Add)]: Column(id=None, name='studyPhase', column_type=STRING, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataType (Add)]: Column(id=None, name='dataType', column_type=STRING_LIST, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_id (Add)]: Column(id='81721', name='id', column_type=ENTITYID, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_description (Add)]: Column(id='87941', name='description', column_type=STRING, facet_type=None, default_value=None, maximum_size=1000, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_createdOn (Add)]: Column(id='81723', name='createdOn', column_type=DATE, facet_type=<FacetType.RANGE: 'range'>, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_createdBy (Add)]: Column(id='81724', name='createdBy', column_type=USERID, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_etag (Add)]: Column(id='81725', name='etag', column_type=STRING, facet_type=None, default_value=None, maximum_size=36, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_modifiedOn (Add)]: Column(id='81726', name='modifiedOn', column_type=DATE, facet_type=<FacetType.RANGE: 'range'>, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_modifiedBy (Add)]: Column(id='81727', name='modifiedBy', column_type=USERID, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_path (Add)]: Column(id='214396', name='path', column_type=STRING, facet_type=None, default_value=None, maximum_size=1000, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_type (Add)]: Column(id='196992', name='type', column_type=STRING, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=20, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_currentVersion (Add)]: Column(id='81729', name='currentVersion', column_type=INTEGER, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_parentId (Add)]: Column(id='81730', name='parentId', column_type=ENTITYID, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_benefactorId (Add)]: Column(id='81731', name='benefactorId', column_type=ENTITYID, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_projectId (Add)]: Column(id='81732', name='projectId', column_type=ENTITYID, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileHandleId (Add)]: Column(id='81733', name='dataFileHandleId', column_type=FILEHANDLEID, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileName (Add)]: Column(id='199088', name='dataFileName', column_type=STRING, facet_type=None, default_value=None, maximum_size=256, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileSizeBytes (Add)]: Column(id='112368', name='dataFileSizeBytes', column_type=INTEGER, facet_type=None, default_value=None, maximum_size=None, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileMD5Hex (Add)]: Column(id='112369', name='dataFileMD5Hex', column_type=STRING, facet_type=None, default_value=None, maximum_size=100, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileConcreteType (Add)]: Column(id='196995', name='dataFileConcreteType', column_type=STRING, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=65, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileBucket (Add)]: Column(id='196996', name='dataFileBucket', column_type=STRING, facet_type=<FacetType.ENUMERATION: 'enumeration'>, default_value=None, maximum_size=100, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View:Column_dataFileKey (Add)]: Column(id='184972', name='dataFileKey', column_type=STRING, facet_type=None, default_value=None, maximum_size=700, maximum_list_length=None, enum_values=None, json_sub_columns=None)\n",
      "[syn71975077:Trehalose_Biomarker_Data_Entity_View]: (Column Order): ['name', 'clinicalDomain', 'studyPhase', 'dataType', 'id', 'description', 'createdOn', 'createdBy', 'etag', 'modifiedOn', 'modifiedBy', 'path', 'type', 'currentVersion', 'parentId', 'benefactorId', 'projectId', 'dataFileHandleId', 'dataFileName', 'dataFileSizeBytes', 'dataFileMD5Hex', 'dataFileConcreteType', 'dataFileBucket', 'dataFileKey']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/entity/syn71975077/table/transaction/async: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00/1.00 [00:01<00:00, 1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created Entity View successfully!\n",
      "   üìã View ID: syn71975077\n",
      "   üè∑Ô∏è Total columns: 4 (name + 3 annotations)\n",
      "   üîó View URL: https://www.synapse.org/#!Synapse:syn71975077\n",
      "\n",
      "‚úÖ Entity View created successfully!\n",
      "   üîó View in Synapse web: https://www.synapse.org/#!Synapse:syn71975077\n",
      "   üìä Query programmatically: syn.query('SELECT * FROM syn71975077')\n",
      "\n",
      "‚úÖ Entity View ID stored in variable: ENTITY_VIEW_ID = 'syn71975077'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: CREATE ENTITY VIEW FOR STAGING FOLDER WITH MINIMAL ANNOTATION COLUMNS\n",
    "\n",
    "from synapseclient.models import EntityView, ViewTypeMask, Column, ColumnType\n",
    "\n",
    "def extract_minimal_annotation_columns(annotations_data):\n",
    "    \"\"\"Extract only the most essential annotation columns that have data.\"\"\"\n",
    "    \n",
    "    print(\"üîç Extracting minimal annotation columns with data...\")\n",
    "    \n",
    "    # Manually specify the most important columns that we know have data\n",
    "    # Based on the annotations file, these are the fields with actual values\n",
    "    essential_columns = [\n",
    "        'clinicalDomain',  # Has values like ['subject_management', 'disease_progression'] \n",
    "        'studyPhase',      # Has values like 'longitudinal'\n",
    "        'dataType',\n",
    "               # Has values like 'clinical' or 'omics'\n",
    "    ]\n",
    "    \n",
    "    columns = []\n",
    "    \n",
    "    # Check which essential columns actually have data\n",
    "    columns_with_data = set()\n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            for col in essential_columns:\n",
    "                if col in annotation:\n",
    "                    value = annotation[col]\n",
    "                    has_data = False\n",
    "                    \n",
    "                    if isinstance(value, list):\n",
    "                        # List field - check if has non-empty values\n",
    "                        meaningful_values = [v for v in value if v and v.strip() != '']\n",
    "                        has_data = len(meaningful_values) > 0\n",
    "                    elif isinstance(value, str):\n",
    "                        # String field - check if non-empty\n",
    "                        has_data = value.strip() != ''\n",
    "                    \n",
    "                    if has_data:\n",
    "                        columns_with_data.add(col)\n",
    "    \n",
    "    print(f\"   üìã Found {len(columns_with_data)} essential columns with data\")\n",
    "    \n",
    "    # Create columns for the ones that have data\n",
    "    for col in essential_columns:\n",
    "        if col in columns_with_data:\n",
    "            # Determine if it's a list type by checking the data\n",
    "            is_list = False\n",
    "            for syn_id, file_data in annotations_data.items():\n",
    "                for file_name, annotation in file_data.items():\n",
    "                    if col in annotation and isinstance(annotation[col], list):\n",
    "                        # Check if it's a meaningful list (not just [''])\n",
    "                        meaningful_values = [v for v in annotation[col] if v and v.strip() != '']\n",
    "                        if len(meaningful_values) > 0:\n",
    "                            is_list = True\n",
    "                            break\n",
    "                if is_list:\n",
    "                    break\n",
    "            \n",
    "            column_type = ColumnType.STRING_LIST if is_list else ColumnType.STRING\n",
    "            columns.append(Column(name=col, column_type=column_type))\n",
    "            print(f\"   ‚ûï {col}: {column_type.value}\")\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(columns)} essential annotation columns\")\n",
    "    return columns\n",
    "\n",
    "def get_or_create_minimal_entity_view(syn, project_id, staging_folder_id, annotations_data, view_name=None):\n",
    "    \"\"\"Create a minimal Entity View for the staging folder with only essential annotation columns.\"\"\"\n",
    "    \n",
    "    if view_name is None:\n",
    "        # Get folder name for view naming\n",
    "        try:\n",
    "            folder = syn.get(staging_folder_id, downloadFile=False)\n",
    "            folder_name = folder.name.replace(' ', '_').replace('-', '_')\n",
    "            view_name = f\"{folder_name}_Entity_View\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not get folder name: {e}\")\n",
    "            view_name = \"Staging_Folder_Entity_View\"\n",
    "    \n",
    "    print(f\"üîç Creating minimal Entity View: {view_name}\")\n",
    "    print(f\"   üìÅ Scope: {staging_folder_id}\")\n",
    "    print(f\"   üèóÔ∏è Project: {project_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if view already exists and delete it\n",
    "        existing_views = syn.getChildren(project_id, includeTypes=['entityview'])\n",
    "        for view_info in existing_views:\n",
    "            if view_info['name'] == view_name:\n",
    "                print(f\"‚ö†Ô∏è Found existing Entity View: {view_name} ({view_info['id']})\")\n",
    "                print(f\"   üóëÔ∏è Deleting existing view to recreate...\")\n",
    "                syn.delete(view_info['id'])\n",
    "                break\n",
    "        \n",
    "        # Extract minimal annotation columns\n",
    "        annotation_columns = extract_minimal_annotation_columns(annotations_data)\n",
    "        \n",
    "        # Create name column first, then annotation columns\n",
    "        all_columns = []\n",
    "        \n",
    "        # Add name column first to ensure it's the first column\n",
    "        all_columns.append(Column(name=\"name\", column_type=ColumnType.STRING))\n",
    "        \n",
    "        # Add annotation columns after name\n",
    "        all_columns.extend(annotation_columns)\n",
    "        \n",
    "        # Create new Entity View with minimal columns\n",
    "        entity_view = EntityView(\n",
    "            name=view_name,\n",
    "            parent_id=project_id,\n",
    "            scope_ids=[staging_folder_id],\n",
    "            view_type_mask=ViewTypeMask.FILE | ViewTypeMask.FOLDER,\n",
    "            columns=all_columns  # Name column first, then annotation columns\n",
    "        )\n",
    "        \n",
    "        # Create the view in Synapse\n",
    "        created_view = entity_view.store()\n",
    "        view_id = created_view.id\n",
    "        \n",
    "        print(f\"‚úÖ Created Entity View successfully!\")\n",
    "        print(f\"   üìã View ID: {view_id}\")\n",
    "        print(f\"   üè∑Ô∏è Total columns: {len(all_columns)} (name + {len(annotation_columns)} annotations)\")\n",
    "        print(f\"   üîó View URL: https://www.synapse.org/#!Synapse:{view_id}\")\n",
    "        \n",
    "        return view_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating Entity View: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute minimal Entity View creation\n",
    "print(\"üìä MINIMAL ENTITY VIEW CREATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Re-establish variables if needed\n",
    "try:\n",
    "    if 'syn' not in globals() or syn is None:\n",
    "        print(\"‚ö†Ô∏è Reconnecting to Synapse...\")\n",
    "        syn = connect_to_synapse()\n",
    "    \n",
    "    if 'PROJECT_ID' not in globals():\n",
    "        PROJECT_ID = \"syn68702804\"\n",
    "    \n",
    "    if 'STAGING_FOLDER_ID' not in globals():\n",
    "        STAGING_FOLDER_ID = \"syn68927891\"\n",
    "    \n",
    "    # Load annotations data if not available\n",
    "    if 'annotations_data' not in globals() or not annotations_data:\n",
    "        if 'annotation_file_path' in globals() and os.path.exists(annotation_file_path):\n",
    "            print(\"üìÇ Loading annotations data...\")\n",
    "            annotations_data = load_existing_annotations(annotation_file_path)\n",
    "        else:\n",
    "            annotations_data = {}\n",
    "        \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing configuration: {e}\")\n",
    "    print(\"üìù Please run previous cells first\")\n",
    "\n",
    "# Create minimal Entity View\n",
    "if syn and PROJECT_ID and STAGING_FOLDER_ID and annotations_data:\n",
    "    view_id = get_or_create_minimal_entity_view(\n",
    "        syn, PROJECT_ID, STAGING_FOLDER_ID, annotations_data\n",
    "    )\n",
    "    \n",
    "    if view_id:\n",
    "        print(f\"\\n‚úÖ Entity View created successfully!\")\n",
    "        print(f\"   üîó View in Synapse web: https://www.synapse.org/#!Synapse:{view_id}\")\n",
    "        print(f\"   üìä Query programmatically: syn.query('SELECT * FROM {view_id}')\")\n",
    "        \n",
    "        # Store view_id for potential use in other cells\n",
    "        ENTITY_VIEW_ID = view_id\n",
    "        print(f\"\\n‚úÖ Entity View ID stored in variable: ENTITY_VIEW_ID = '{view_id}'\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not create Entity View\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not syn: missing.append(\"syn\")\n",
    "    if not PROJECT_ID: missing.append(\"PROJECT_ID\") \n",
    "    if not STAGING_FOLDER_ID: missing.append(\"STAGING_FOLDER_ID\")\n",
    "    if not annotations_data: missing.append(\"annotations_data\")\n",
    "    \n",
    "    print(f\"‚ùå Missing required variables: {', '.join(missing)}\")\n",
    "    print(\"üìù Please run previous cells to set up these variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ntxgdo7ba2p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATASET ANNOTATION MANAGEMENT\n",
      "==============================\n",
      "üìÑ Dataset annotation file: ../annotations/trehalose_biomarker_data_dataset_annotations.json\n",
      "üìã No existing dataset annotations found\n",
      "üîç Detecting dataset type from staging folder content...\n",
      "   üìä Detected: Omic dataset (16 omic files vs 7 clinical files)\n",
      "  OmicDataset inherits from BaseDataset\n",
      "üìù Created new omic dataset annotation template\n",
      "üíæ Dataset annotations saved to: ../annotations/trehalose_biomarker_data_dataset_annotations.json\n",
      "\n",
      "üîç Validating dataset annotations...\n",
      "  OmicDataset inherits from BaseDataset\n",
      "‚ùå Dataset annotation validation failed:\n",
      "   ‚Ä¢ Required field 'title' is empty\n",
      "   ‚Ä¢ Required field 'creator' is empty\n",
      "   ‚Ä¢ Required field 'keywords' is empty\n",
      "   ‚Ä¢ Required field 'source' is empty\n",
      "   ‚Ä¢ Required field 'url' is empty\n",
      "\n",
      "üìù Please fill out required fields in: ../annotations/trehalose_biomarker_data_dataset_annotations.json\n",
      "üîÑ Re-run this cell after making changes\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: DATASET ANNOTATION MANAGEMENT\n",
    "\n",
    "from synapseclient.models import Dataset\n",
    "\n",
    "def create_dataset_annotation_template(dataset_type, all_schemas):\n",
    "    \"\"\"Create blank dataset annotation template based on dataset type.\"\"\"\n",
    "    \n",
    "    # Choose appropriate dataset schema\n",
    "    if dataset_type.lower() == 'omic':\n",
    "        schema_attributes = get_full_schema('OmicDataset', all_schemas)\n",
    "        dataset_class = 'OmicDataset'\n",
    "    else:\n",
    "        schema_attributes = get_full_schema('ClinicalDataset', all_schemas)  \n",
    "        dataset_class = 'ClinicalDataset'\n",
    "    \n",
    "    # Build template from schema\n",
    "    template = {}\n",
    "    for attr_name, attr_def in schema_attributes.items():\n",
    "        # Handle multivalued attributes\n",
    "        if isinstance(attr_def, dict) and attr_def.get('multivalued', False):\n",
    "            template[attr_name] = ['']\n",
    "        else:\n",
    "            template[attr_name] = ''\n",
    "    \n",
    "    # Add metadata about detected dataset type\n",
    "    template['_dataset_type'] = dataset_class\n",
    "    template['_schema_source'] = 'data-model'\n",
    "    template['_created_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    return template\n",
    "\n",
    "def create_dataset_annotations_file_path(staging_folder_name):\n",
    "    \"\"\"Create path: ../annotations/{folder_name}_dataset_annotations.json\"\"\"\n",
    "    clean_name = staging_folder_name.lower().replace(' ', '_').replace('-', '_')\n",
    "    clean_name = re.sub(r'[^a-z0-9_]', '', clean_name)  # Remove special chars\n",
    "    return f\"{ANNOTATIONS_DIR}/{clean_name}_dataset_annotations.json\"\n",
    "\n",
    "def smart_merge_dataset_annotations(existing_annotations, new_template):\n",
    "    \"\"\"\n",
    "    Smart merge that:\n",
    "    1. Keeps existing filled annotations intact\n",
    "    2. Only adds new attributes if they don't exist\n",
    "    3. Does not overwrite any existing filled values\n",
    "    \"\"\"\n",
    "    if not existing_annotations:\n",
    "        return new_template\n",
    "    \n",
    "    merged = existing_annotations.copy()\n",
    "    added_count = 0\n",
    "    \n",
    "    for key, value in new_template.items():\n",
    "        if key not in merged:\n",
    "            # New field - add it\n",
    "            merged[key] = value\n",
    "            added_count += 1\n",
    "        else:\n",
    "            # Field exists - only update if current value is empty\n",
    "            current_value = merged[key]\n",
    "            is_empty = False\n",
    "            \n",
    "            if isinstance(current_value, list):\n",
    "                is_empty = len([v for v in current_value if v and v.strip() != '']) == 0\n",
    "            elif isinstance(current_value, str):\n",
    "                is_empty = current_value.strip() == ''\n",
    "            else:\n",
    "                is_empty = current_value is None\n",
    "            \n",
    "            if is_empty and value:\n",
    "                merged[key] = value\n",
    "                added_count += 1\n",
    "    \n",
    "    print(f\"üìä Dataset annotation merge: {added_count} new/updated fields\")\n",
    "    return merged\n",
    "\n",
    "def validate_dataset_annotation(annotation, all_schemas):\n",
    "    \"\"\"Validate dataset annotation against its schema.\"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    dataset_type = annotation.get('_dataset_type', 'ClinicalDataset')\n",
    "    \n",
    "    # Get expected schema\n",
    "    if dataset_type == 'OmicDataset':\n",
    "        expected_schema = get_full_schema('OmicDataset', all_schemas)\n",
    "    else:\n",
    "        expected_schema = get_full_schema('ClinicalDataset', all_schemas)\n",
    "    \n",
    "    # Check for required fields\n",
    "    for attr_name, attr_def in expected_schema.items():\n",
    "        if isinstance(attr_def, dict) and attr_def.get('required', False):\n",
    "            if attr_name not in annotation:\n",
    "                errors.append(f\"Missing required field: {attr_name}\")\n",
    "            elif not annotation[attr_name] or annotation[attr_name] == '' or annotation[attr_name] == ['']:\n",
    "                errors.append(f\"Required field '{attr_name}' is empty\")\n",
    "    \n",
    "    # Check multivalued field constraints\n",
    "    for attr_name, value in annotation.items():\n",
    "        if attr_name.startswith('_'):  # Skip metadata fields\n",
    "            continue\n",
    "            \n",
    "        if attr_name in expected_schema:\n",
    "            attr_def = expected_schema[attr_name]\n",
    "            if isinstance(attr_def, dict):\n",
    "                is_multivalued = attr_def.get('multivalued', False)\n",
    "                \n",
    "                if is_multivalued and not isinstance(value, list):\n",
    "                    errors.append(f\"Field '{attr_name}' should be a list (multivalued)\")\n",
    "                elif not is_multivalued and isinstance(value, list):\n",
    "                    warnings.append(f\"Field '{attr_name}' is a list but should be single value\")\n",
    "        else:\n",
    "            warnings.append(f\"Field '{attr_name}' not found in schema (may be deprecated)\")\n",
    "    \n",
    "    # Check for completely empty annotations\n",
    "    non_metadata_fields = {k: v for k, v in annotation.items() if not k.startswith('_')}\n",
    "    filled_fields = {\n",
    "        k: v for k, v in non_metadata_fields.items() \n",
    "        if v and v != '' and v != [''] and v != []\n",
    "    }\n",
    "    \n",
    "    if len(filled_fields) < 3:  # At least title, description, dataType should be filled\n",
    "        warnings.append(\"Very few fields have been filled out - consider adding more metadata\")\n",
    "    \n",
    "    return len(errors) == 0, errors, warnings\n",
    "\n",
    "def create_dataset_entity(syn, project_id, dataset_name, dataset_annotations):\n",
    "    \"\"\"Create a Dataset entity in Synapse with annotations.\"\"\"\n",
    "    try:\n",
    "        # Clean annotations for Synapse (remove metadata fields)\n",
    "        clean_annotations = {}\n",
    "        for key, value in dataset_annotations.items():\n",
    "            if not key.startswith('_'):  # Skip metadata fields\n",
    "                # Only include non-empty values\n",
    "                if isinstance(value, list):\n",
    "                    cleaned_list = [v for v in value if v and v.strip() != '']\n",
    "                    if cleaned_list:\n",
    "                        clean_annotations[key] = cleaned_list\n",
    "                elif isinstance(value, str) and value.strip() != '':\n",
    "                    clean_annotations[key] = value\n",
    "                elif value is not None:\n",
    "                    clean_annotations[key] = value\n",
    "        \n",
    "        # Create Dataset entity\n",
    "        dataset = Dataset(\n",
    "            name=dataset_name,\n",
    "            parent_id=project_id,\n",
    "            dataset_items=[],  # Start empty, files can be added later\n",
    "            annotations=clean_annotations\n",
    "        )\n",
    "        \n",
    "        # Store the dataset\n",
    "        created_dataset = dataset.store()\n",
    "        \n",
    "        print(f\"‚úÖ Created Dataset entity successfully!\")\n",
    "        print(f\"   üìã Dataset ID: {created_dataset.id}\")\n",
    "        print(f\"   üìõ Dataset Name: {dataset_name}\")\n",
    "        print(f\"   üè∑Ô∏è Annotations Applied: {len(clean_annotations)}\")\n",
    "        print(f\"   üîó Dataset URL: https://www.synapse.org/#!Synapse:{created_dataset.id}\")\n",
    "        \n",
    "        return created_dataset.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating Dataset entity: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute dataset annotation management\n",
    "print(\"üìä DATASET ANNOTATION MANAGEMENT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Re-establish variables if needed\n",
    "try:\n",
    "    if 'staging_folder_name' not in globals():\n",
    "        staging_folder_name = \"trehalose_biomarker_data\"  # fallback\n",
    "    \n",
    "    if 'all_schemas' not in globals() or not all_schemas:\n",
    "        print(\"üîç Loading schemas...\")\n",
    "        all_schemas = get_all_schemas()\n",
    "        \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing dependencies: {e}\")\n",
    "    print(\"üìù Please run previous cells first\")\n",
    "\n",
    "if all_schemas:\n",
    "    # Create dataset annotations file path\n",
    "    dataset_annotation_file_path = create_dataset_annotations_file_path(staging_folder_name)\n",
    "    print(f\"üìÑ Dataset annotation file: {dataset_annotation_file_path}\")\n",
    "    \n",
    "    # Load existing dataset annotations if they exist\n",
    "    existing_dataset_annotations = load_existing_annotations(dataset_annotation_file_path)\n",
    "    \n",
    "    if existing_dataset_annotations:\n",
    "        print(f\"üìÇ Found existing dataset annotations with {len(existing_dataset_annotations)} fields\")\n",
    "        print(f\"   üìù Edit file to update values: {dataset_annotation_file_path}\")\n",
    "        \n",
    "        # Detect dataset type from existing annotations\n",
    "        dataset_type = existing_dataset_annotations.get('_dataset_type', 'ClinicalDataset')\n",
    "        if 'Omic' in dataset_type:\n",
    "            template_type = 'omic'\n",
    "        else:\n",
    "            template_type = 'clinical'\n",
    "            \n",
    "        # Create template and merge with existing\n",
    "        new_template = create_dataset_annotation_template(template_type, all_schemas)\n",
    "        updated_dataset_annotations = smart_merge_dataset_annotations(existing_dataset_annotations, new_template)\n",
    "        \n",
    "    else:\n",
    "        print(\"üìã No existing dataset annotations found\")\n",
    "        print(\"üîç Detecting dataset type from staging folder content...\")\n",
    "        \n",
    "        # Try to detect dataset type from file annotations\n",
    "        dataset_type = 'clinical'  # default\n",
    "        if 'annotations_data' in globals() and annotations_data:\n",
    "            # Check file types to infer dataset type\n",
    "            omic_files = 0\n",
    "            clinical_files = 0\n",
    "            \n",
    "            for syn_id, file_data in annotations_data.items():\n",
    "                for file_name, annotation in file_data.items():\n",
    "                    file_type = annotation.get('_file_type', '')\n",
    "                    if 'Omic' in file_type:\n",
    "                        omic_files += 1\n",
    "                    else:\n",
    "                        clinical_files += 1\n",
    "            \n",
    "            if omic_files > clinical_files:\n",
    "                dataset_type = 'omic'\n",
    "                print(f\"   üìä Detected: Omic dataset ({omic_files} omic files vs {clinical_files} clinical files)\")\n",
    "            else:\n",
    "                dataset_type = 'clinical'\n",
    "                print(f\"   üìä Detected: Clinical dataset ({clinical_files} clinical files vs {omic_files} omic files)\")\n",
    "        \n",
    "        # Create new template\n",
    "        updated_dataset_annotations = create_dataset_annotation_template(dataset_type, all_schemas)\n",
    "        print(f\"üìù Created new {dataset_type} dataset annotation template\")\n",
    "    \n",
    "    # Save updated annotations\n",
    "    save_annotations(updated_dataset_annotations, dataset_annotation_file_path)\n",
    "    print(f\"üíæ Dataset annotations saved to: {dataset_annotation_file_path}\")\n",
    "    \n",
    "    # Validate the dataset annotations\n",
    "    print(f\"\\nüîç Validating dataset annotations...\")\n",
    "    is_valid, errors, warnings = validate_dataset_annotation(updated_dataset_annotations, all_schemas)\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"‚ùå Dataset annotation validation failed:\")\n",
    "        for error in errors:\n",
    "            print(f\"   ‚Ä¢ {error}\")\n",
    "        print(f\"\\nüìù Please fill out required fields in: {dataset_annotation_file_path}\")\n",
    "        print(f\"üîÑ Re-run this cell after making changes\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Dataset annotation validation passed!\")\n",
    "        if warnings:\n",
    "            print(f\"‚ö†Ô∏è  {len(warnings)} warnings:\")\n",
    "            for warning in warnings[:3]:  # Show first 3\n",
    "                print(f\"   ‚Ä¢ {warning}\")\n",
    "        \n",
    "        # Check if dataset name is filled out for entity creation\n",
    "        dataset_name = updated_dataset_annotations.get('title', '').strip()\n",
    "        if not dataset_name:\n",
    "            print(f\"\\nüìù To create Dataset entity, please add a 'title' field to the annotations file\")\n",
    "        else:\n",
    "            print(f\"\\nüéØ Dataset is ready for entity creation!\")\n",
    "            print(f\"   üìõ Dataset Name: {dataset_name}\")\n",
    "            print(f\"   üîß To create Dataset entity, run a separate cell with:\")\n",
    "            print(f\"      create_dataset_entity(syn, PROJECT_ID, '{dataset_name}', updated_dataset_annotations)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot create dataset annotations: missing schemas\")\n",
    "    print(\"üìù Please run Cell 4 (schema loading) first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e1zl9xeyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: CREATE DATASET ENTITY\n",
    "\n",
    "def create_dataset_entity_with_validation(syn, project_id):\n",
    "    \"\"\"Create Dataset entity after validating annotations and getting user input.\"\"\"\n",
    "    \n",
    "    # Re-establish variables if needed\n",
    "    if 'dataset_annotation_file_path' not in globals():\n",
    "        if 'staging_folder_name' in globals():\n",
    "            dataset_annotation_file_path = create_dataset_annotations_file_path(staging_folder_name)\n",
    "        else:\n",
    "            print(\"‚ùå Missing staging folder name - please run previous cells\")\n",
    "            return None\n",
    "    \n",
    "    # Load dataset annotations\n",
    "    try:\n",
    "        dataset_annotations = load_existing_annotations(dataset_annotation_file_path)\n",
    "        if not dataset_annotations:\n",
    "            print(f\"‚ùå No dataset annotations found at: {dataset_annotation_file_path}\")\n",
    "            print(\"üìù Please run Cell 9 (Dataset Annotation Management) first\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset annotations: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Validate dataset annotations\n",
    "    print(\"üîç Validating dataset annotations before entity creation...\")\n",
    "    if 'all_schemas' not in globals():\n",
    "        print(\"üîç Loading schemas...\")\n",
    "        all_schemas = get_all_schemas()\n",
    "    \n",
    "    is_valid, errors, warnings = validate_dataset_annotation(dataset_annotations, all_schemas)\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"‚ùå Dataset annotation validation failed:\")\n",
    "        for error in errors:\n",
    "            print(f\"   ‚Ä¢ {error}\")\n",
    "        print(f\"\\nüìù Please fix errors in: {dataset_annotation_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"‚úÖ Dataset annotations are valid!\")\n",
    "    if warnings:\n",
    "        print(f\"‚ö†Ô∏è  {len(warnings)} warnings (non-blocking)\")\n",
    "    \n",
    "    # Get dataset name from annotations\n",
    "    dataset_name = dataset_annotations.get('title', '').strip()\n",
    "    if not dataset_name:\n",
    "        print(\"‚ùå Dataset 'title' field is required for entity creation\")\n",
    "        print(f\"üìù Please add a title in: {dataset_annotation_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if dataset already exists\n",
    "    print(f\"\\nüîç Checking for existing datasets with name: '{dataset_name}'\")\n",
    "    try:\n",
    "        existing_datasets = syn.getChildren(project_id, includeTypes=['dataset'])\n",
    "        for dataset_info in existing_datasets:\n",
    "            if dataset_info['name'] == dataset_name:\n",
    "                print(f\"‚ö†Ô∏è  Dataset already exists: {dataset_name} ({dataset_info['id']})\")\n",
    "                print(f\"   üîó URL: https://www.synapse.org/#!Synapse:{dataset_info['id']}\")\n",
    "                \n",
    "                # Ask user what to do\n",
    "                print(f\"\\n‚ùì What would you like to do?\")\n",
    "                print(f\"   1. Skip creation (use existing dataset)\")\n",
    "                print(f\"   2. Update existing dataset annotations\")\n",
    "                print(f\"   3. Create new dataset with different name\")\n",
    "                \n",
    "                # For notebook use, we'll skip creation and show the existing dataset\n",
    "                print(f\"üìù Skipping creation - existing dataset found\")\n",
    "                return dataset_info['id']\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not check for existing datasets: {e}\")\n",
    "    \n",
    "    # Create new Dataset entity\n",
    "    print(f\"\\nüîÑ Creating Dataset entity: '{dataset_name}'\")\n",
    "    dataset_id = create_dataset_entity(syn, project_id, dataset_name, dataset_annotations)\n",
    "    \n",
    "    if dataset_id:\n",
    "        # Store dataset info for future use\n",
    "        dataset_info = {\n",
    "            'id': dataset_id,\n",
    "            'name': dataset_name,\n",
    "            'annotation_file': dataset_annotation_file_path,\n",
    "            'created_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save dataset info to a file for tracking\n",
    "        dataset_info_file = f\"{ANNOTATIONS_DIR}/dataset_info.json\"\n",
    "        try:\n",
    "            with open(dataset_info_file, 'w') as f:\n",
    "                json.dump(dataset_info, f, indent=2)\n",
    "            print(f\"üíæ Dataset info saved to: {dataset_info_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not save dataset info: {e}\")\n",
    "        \n",
    "        return dataset_id\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Execute Dataset entity creation\n",
    "print(\"üèóÔ∏è  DATASET ENTITY CREATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Re-establish variables if needed\n",
    "try:\n",
    "    if 'syn' not in globals() or syn is None:\n",
    "        print(\"‚ö†Ô∏è Reconnecting to Synapse...\")\n",
    "        syn = connect_to_synapse()\n",
    "    \n",
    "    if 'PROJECT_ID' not in globals():\n",
    "        PROJECT_ID = \"syn68702804\"\n",
    "        \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing configuration: {e}\")\n",
    "    print(\"üìù Please run previous cells first\")\n",
    "\n",
    "# Create Dataset entity\n",
    "if syn and PROJECT_ID:\n",
    "    dataset_id = create_dataset_entity_with_validation(syn, PROJECT_ID)\n",
    "    \n",
    "    if dataset_id:\n",
    "        print(f\"\\nüéâ Dataset entity ready!\")\n",
    "        print(f\"   üìã Dataset ID: {dataset_id}\")\n",
    "        print(f\"   üîó View Dataset: https://www.synapse.org/#!Synapse:{dataset_id}\")\n",
    "        \n",
    "        # Store for use in other cells\n",
    "        DATASET_ID = dataset_id\n",
    "        print(f\"\\n‚úÖ Dataset ID stored in variable: DATASET_ID = '{dataset_id}'\")\n",
    "        \n",
    "        print(f\"\\nüí° Next steps:\")\n",
    "        print(f\"   üìÅ Add files to dataset using dataset.dataset_items\")\n",
    "        print(f\"   üè∑Ô∏è  Update annotations as needed\")\n",
    "        print(f\"   üìä Create additional Entity Views if needed\")\n",
    "    else:\n",
    "        print(\"‚ùå Dataset entity creation failed\")\n",
    "else:\n",
    "    print(\"‚ùå Missing required variables: syn, PROJECT_ID\")\n",
    "    print(\"üìù Please run previous cells to set up these variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3592aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: ADD FILES TO DATASET ENTITY\n",
    "\n",
    "from synapseclient.models import Folder\n",
    "\n",
    "def add_staging_folder_to_dataset(syn, dataset_id, staging_folder_id):\n",
    "    \"\"\"Add all files from staging folder to the dataset entity.\"\"\"\n",
    "    try:\n",
    "        print(f\"üìÇ Adding staging folder contents to dataset...\")\n",
    "        print(f\"   üìã Dataset ID: {dataset_id}\")\n",
    "        print(f\"   üìÅ Staging Folder ID: {staging_folder_id}\")\n",
    "        \n",
    "        # Get the dataset entity\n",
    "        dataset=Dataset(id=dataset_id).get() \n",
    "        \n",
    "        # Get initial item count\n",
    "        initial_count = len(dataset.dataset_items) if hasattr(dataset, 'dataset_items') else 0\n",
    "        print(f\"   üìä Current dataset items: {initial_count}\")\n",
    "        \n",
    "        # Add the entire staging folder (this recursively adds all child files)\n",
    "        print(f\"\\nüîÑ Adding folder to dataset...\")\n",
    "        dataset.add_item(Folder(id=staging_folder_id))\n",
    "        \n",
    "        # Store the changes to Synapse\n",
    "        print(f\"üíæ Saving changes to Synapse...\")\n",
    "        updated_dataset = dataset.store()\n",
    "        \n",
    "        # Get updated item count\n",
    "        final_count = len(updated_dataset.dataset_items) if hasattr(updated_dataset, 'dataset_items') else 0\n",
    "        added_count = final_count - initial_count\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully added staging folder to dataset!\")\n",
    "        print(f\"   üìä Total dataset items: {final_count}\")\n",
    "        print(f\"   ‚ûï Files added: {added_count}\")\n",
    "        print(f\"   üîó Dataset URL: https://www.synapse.org/#!Synapse:{dataset_id}\")\n",
    "        \n",
    "        # List some of the items\n",
    "        if final_count > 0:\n",
    "            print(f\"\\nüìã Dataset items preview:\")\n",
    "            for idx, item in enumerate(updated_dataset.dataset_items[:5]):\n",
    "                entity_id = item.entity_id if hasattr(item, 'entity_id') else 'unknown'\n",
    "                version = item.version_number if hasattr(item, 'version_number') else 'latest'\n",
    "                print(f\"   {idx+1}. {entity_id} (v{version})\")\n",
    "            \n",
    "            if final_count > 5:\n",
    "                print(f\"   ... and {final_count - 5} more items\")\n",
    "        \n",
    "        return updated_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding staging folder to dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def verify_dataset_contents(syn, dataset_id):\n",
    "    \"\"\"Verify the dataset contents and annotations.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüîç DATASET VERIFICATION\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # Get the dataset\n",
    "        dataset=Dataset(id=dataset_id).get()\n",
    "        \n",
    "        print(f\"üìã Dataset: {dataset.name}\")\n",
    "        print(f\"   ID: {dataset.id}\")\n",
    "        print(f\"   Parent: {dataset.parent_id}\")\n",
    "        \n",
    "        # Check items\n",
    "        item_count = len(dataset.dataset_items) if hasattr(dataset, 'dataset_items') else 0\n",
    "        print(f\"\\nüìä Dataset Items: {item_count}\")\n",
    "        \n",
    "        # Check annotations\n",
    "        annotations = dataset.annotations if hasattr(dataset, 'annotations') else {}\n",
    "        print(f\"üè∑Ô∏è  Annotations: {len(annotations)}\")\n",
    "        \n",
    "        if annotations:\n",
    "            print(f\"\\n   Key annotations:\")\n",
    "            # Show some important annotations\n",
    "            important_keys = ['title', 'description', 'dataType', 'studyPhase', 'disease', 'dataFormat']\n",
    "            for key in important_keys:\n",
    "                if key in annotations:\n",
    "                    value = annotations[key]\n",
    "                    if isinstance(value, list):\n",
    "                        display_value = ', '.join([str(v) for v in value[:3]])\n",
    "                        if len(value) > 3:\n",
    "                            display_value += f\" (+{len(value)-3} more)\"\n",
    "                    else:\n",
    "                        display_value = str(value)[:60]\n",
    "                    print(f\"   ‚Ä¢ {key}: {display_value}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset verification complete!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error verifying dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "# Execute: Add files to dataset\n",
    "print(\"üèóÔ∏è  ADD FILES TO DATASET\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Re-establish variables if needed\n",
    "try:\n",
    "    if 'syn' not in globals() or syn is None:\n",
    "        print(\"‚ö†Ô∏è Reconnecting to Synapse...\")\n",
    "        syn = connect_to_synapse()\n",
    "    \n",
    "    if 'STAGING_FOLDER_ID' not in globals():\n",
    "        STAGING_FOLDER_ID = \"syn68927891\"  # fallback\n",
    "    \n",
    "    # Check if we have a dataset_id from the previous cell\n",
    "    if 'dataset_id' not in globals() or dataset_id is None:\n",
    "        print(\"‚ùå No dataset_id found from previous cell\")\n",
    "        print(\"üìù Please run Cell 12 (Dataset Entity Creation) first\")\n",
    "    else:\n",
    "        print(f\"üìã Using dataset from previous cell: {dataset_id}\")\n",
    "        \n",
    "        # Add staging folder to dataset\n",
    "        updated_dataset = add_staging_folder_to_dataset(\n",
    "            syn, \n",
    "            dataset_id, \n",
    "            STAGING_FOLDER_ID\n",
    "        )\n",
    "        \n",
    "        if updated_dataset:\n",
    "            # Verify the dataset\n",
    "            verify_dataset_contents(syn, dataset_id)\n",
    "        else:\n",
    "            print(\"‚ùå Could not add files to dataset\")\n",
    "            \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing configuration: {e}\")\n",
    "    print(\"üìù Please run previous cells first\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff57b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: ADD DATASET COLUMNS FOR FACETED SEARCH\n",
    "\n",
    "from synapseclient.models import Column, ColumnType, FacetType, Dataset\n",
    "\n",
    "def get_dataset_column_schema(dataset_type):\n",
    "    \"\"\"Get column schema based on dataset type (Clinical or Omic).\"\"\"\n",
    "    \n",
    "    # Shared columns for both clinical and omic datasets\n",
    "    # Note: Set maximum_size and maximum_list_length to stay under 64KB row limit\n",
    "    shared_columns = [\n",
    "        {\"name\": \"dataType\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Data type\"},\n",
    "        {\"name\": \"fileFormat\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 50, \"desc\": \"File format\"},\n",
    "        {\"name\": \"species\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Species\"},\n",
    "        {\"name\": \"disease\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Disease\"},\n",
    "        {\"name\": \"studyType\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Study type\"},\n",
    "        {\"name\": \"dataFormat\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 10, \"desc\": \"Data format(s)\"},\n",
    "    ]\n",
    "    \n",
    "    # Clinical-specific columns\n",
    "    clinical_columns = [\n",
    "        {\"name\": \"studyPhase\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Phase of study\"},\n",
    "        {\"name\": \"keyMeasures\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 20, \"desc\": \"Key measurements\"},\n",
    "        {\"name\": \"assessmentType\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 15, \"desc\": \"Type of assessment\"},\n",
    "        {\"name\": \"clinicalDomain\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 15, \"desc\": \"Clinical domain\"},\n",
    "        {\"name\": \"hasLongitudinalData\", \"type\": ColumnType.BOOLEAN, \"facet\": FacetType.ENUMERATION, \"desc\": \"Contains longitudinal data\"},\n",
    "        {\"name\": \"studyDesign\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 150, \"desc\": \"Study design type\"},\n",
    "        {\"name\": \"primaryOutcome\", \"type\": ColumnType.STRING, \"facet\": None, \"max_size\": 250, \"desc\": \"Primary outcome measure\"},\n",
    "    ]\n",
    "    \n",
    "    # Omic-specific columns\n",
    "    omic_columns = [\n",
    "        {\"name\": \"assay\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 10, \"desc\": \"Assay type(s)\"},\n",
    "        {\"name\": \"platform\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 150, \"desc\": \"Sequencing/analysis platform\"},\n",
    "        {\"name\": \"libraryStrategy\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Library strategy\"},\n",
    "        {\"name\": \"libraryLayout\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 50, \"desc\": \"Library layout\"},\n",
    "        {\"name\": \"cellType\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 10, \"desc\": \"Cell type(s)\"},\n",
    "        {\"name\": \"biospecimenType\", \"type\": ColumnType.STRING_LIST, \"facet\": FacetType.ENUMERATION, \"max_list_len\": 10, \"desc\": \"Biospecimen type(s)\"},\n",
    "        {\"name\": \"processingLevel\", \"type\": ColumnType.STRING, \"facet\": FacetType.ENUMERATION, \"max_size\": 100, \"desc\": \"Data processing level\"},\n",
    "    ]\n",
    "    \n",
    "    # Combine columns based on dataset type\n",
    "    if dataset_type and 'omic' in dataset_type.lower():\n",
    "        return shared_columns + omic_columns\n",
    "    else:\n",
    "        return shared_columns + clinical_columns\n",
    "\n",
    "def add_dataset_columns(syn, dataset_id, dataset_type=None):\n",
    "    \"\"\"Add annotation columns to dataset for faceted search.\"\"\"\n",
    "    try:\n",
    "        print(f\"üîß Adding columns to dataset for faceted search...\")\n",
    "        print(f\"   üìã Dataset ID: {dataset_id}\")\n",
    "        print(f\"   üìä Dataset Type: {dataset_type or 'Auto-detect'}\")\n",
    "        \n",
    "        # Get the dataset using Dataset model (not syn.get)\n",
    "        print(f\"   üîÑ Loading dataset...\")\n",
    "        dataset = Dataset(id=dataset_id).get()\n",
    "        \n",
    "        # Auto-detect dataset type if not provided\n",
    "        if not dataset_type:\n",
    "            annotations = dataset.annotations if hasattr(dataset, 'annotations') else {}\n",
    "            dataset_type = annotations.get('_dataset_type', 'ClinicalDataset')\n",
    "            print(f\"   üîç Auto-detected type: {dataset_type}\")\n",
    "        \n",
    "        # Get column schema for this dataset type\n",
    "        columns_to_add = get_dataset_column_schema(dataset_type)\n",
    "        \n",
    "        # Get existing columns\n",
    "        existing_columns = []\n",
    "        if hasattr(dataset, 'columns_to_store') and dataset.columns_to_store:\n",
    "            existing_columns = [col.name for col in dataset.columns_to_store]\n",
    "        \n",
    "        print(f\"   üìä Existing columns: {len(existing_columns)}\")\n",
    "        print(f\"   ‚ûï Columns to add: {len(columns_to_add)}\")\n",
    "        \n",
    "        # Add new columns\n",
    "        new_columns = []\n",
    "        for col_info in columns_to_add:\n",
    "            if col_info['name'] not in existing_columns:\n",
    "                new_columns.append(col_info)\n",
    "        \n",
    "        if new_columns:\n",
    "            print(f\"\\nüîÑ Adding {len(new_columns)} new columns...\")\n",
    "            for col_info in new_columns:\n",
    "                try:\n",
    "                    # Build column with size constraints\n",
    "                    col_kwargs = {\n",
    "                        'name': col_info['name'],\n",
    "                        'column_type': col_info['type'],\n",
    "                        'facet_type': col_info.get('facet')\n",
    "                    }\n",
    "                    \n",
    "                    # Add size constraints based on column type\n",
    "                    if col_info['type'] == ColumnType.STRING and 'max_size' in col_info:\n",
    "                        col_kwargs['maximum_size'] = col_info['max_size']\n",
    "                    elif col_info['type'] == ColumnType.STRING_LIST and 'max_list_len' in col_info:\n",
    "                        col_kwargs['maximum_list_length'] = col_info['max_list_len']\n",
    "                    \n",
    "                    col = Column(**col_kwargs)\n",
    "                    dataset.add_column(column=col)\n",
    "                    \n",
    "                    # Show size info\n",
    "                    size_info = ''\n",
    "                    if 'max_size' in col_info:\n",
    "                        size_info = f\" (max: {col_info['max_size']})\"\n",
    "                    elif 'max_list_len' in col_info:\n",
    "                        size_info = f\" (max list: {col_info['max_list_len']})\"\n",
    "                    \n",
    "                    print(f\"   ‚úì {col_info['name']}: {col_info['type'].value}{size_info}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚úó {col_info['name']}: {e}\")\n",
    "            \n",
    "            # Store the dataset to persist columns using Dataset model's store method\n",
    "            print(f\"\\nüíæ Saving columns to dataset...\")\n",
    "            updated_dataset = dataset.store()\n",
    "            \n",
    "            # Verify columns were added\n",
    "            final_column_count = len(updated_dataset.columns_to_store) if hasattr(updated_dataset, 'columns_to_store') else 0\n",
    "            \n",
    "            print(f\"\\n‚úÖ Successfully added columns to dataset!\")\n",
    "            print(f\"   üìä Total columns: {final_column_count}\")\n",
    "            print(f\"   ‚ûï New columns: {len(new_columns)}\")\n",
    "            print(f\"   üîó Dataset URL: https://www.synapse.org/#!Synapse:{dataset_id}\")\n",
    "            \n",
    "            return updated_dataset\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ All required columns already exist!\")\n",
    "            print(f\"   üìä Total columns: {len(existing_columns)}\")\n",
    "            return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding columns to dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def verify_dataset_columns(syn, dataset_id):\n",
    "    \"\"\"Verify dataset columns were added correctly.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüîç COLUMN VERIFICATION\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # Get the dataset using Dataset model with include_columns=True\n",
    "        dataset = Dataset(id=dataset_id).get(include_columns=True)\n",
    "        \n",
    "        if hasattr(dataset, 'columns_to_store') and dataset.columns_to_store:\n",
    "            columns = dataset.columns_to_store\n",
    "            print(f\"üìä Total columns: {len(columns)}\")\n",
    "            \n",
    "            # Group columns by facet type\n",
    "            faceted_columns = [c for c in columns if c.facet_type]\n",
    "            non_faceted_columns = [c for c in columns if not c.facet_type]\n",
    "            \n",
    "            print(f\"\\n   Faceted columns (searchable): {len(faceted_columns)}\")\n",
    "            print(f\"   Non-faceted columns: {len(non_faceted_columns)}\")\n",
    "            \n",
    "            # Show sample of faceted columns\n",
    "            print(f\"\\n   Sample faceted columns:\")\n",
    "            for col in faceted_columns[:10]:\n",
    "                facet_display = col.facet_type.value if col.facet_type else 'None'\n",
    "                # Show size constraints\n",
    "                size_info = ''\n",
    "                if col.maximum_size:\n",
    "                    size_info = f\" (max: {col.maximum_size})\"\n",
    "                elif col.maximum_list_length:\n",
    "                    size_info = f\" (max list: {col.maximum_list_length})\"\n",
    "                print(f\"   ‚Ä¢ {col.name}: {col.column_type.value}{size_info}\")\n",
    "            \n",
    "            if len(faceted_columns) > 10:\n",
    "                print(f\"   ... and {len(faceted_columns) - 10} more faceted columns\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No columns found on dataset\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Column verification complete!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error verifying columns: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Execute: Add columns to dataset\n",
    "print(\"üèóÔ∏è  ADD DATASET COLUMNS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Re-establish variables if needed\n",
    "try:\n",
    "    if 'syn' not in globals() or syn is None:\n",
    "        print(\"‚ö†Ô∏è Reconnecting to Synapse...\")\n",
    "        syn = connect_to_synapse()\n",
    "    \n",
    "    # Check if we have a dataset_id from previous cells\n",
    "    if 'dataset_id' not in globals() or dataset_id is None:\n",
    "        print(\"‚ùå No dataset_id found from previous cells\")\n",
    "        print(\"üìù Please run Cell 12 (Dataset Entity Creation) first\")\n",
    "    else:\n",
    "        print(f\"üìã Using dataset from previous cell: {dataset_id}\")\n",
    "        \n",
    "        # Load dataset annotations to get type\n",
    "        if 'dataset_annotation_file_path' in globals() and os.path.exists(dataset_annotation_file_path):\n",
    "            dataset_annotations = load_existing_annotations(dataset_annotation_file_path)\n",
    "            dataset_type = dataset_annotations.get('_dataset_type', 'ClinicalDataset')\n",
    "        else:\n",
    "            dataset_type = None  # Will auto-detect\n",
    "        \n",
    "        # Add columns to dataset\n",
    "        updated_dataset = add_dataset_columns(\n",
    "            syn, \n",
    "            dataset_id,\n",
    "            dataset_type=dataset_type\n",
    "        )\n",
    "        \n",
    "        if updated_dataset:\n",
    "            # Verify the columns\n",
    "            verify_dataset_columns(syn, dataset_id)\n",
    "        else:\n",
    "            print(\"‚ùå Could not add columns to dataset\")\n",
    "            \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing configuration: {e}\")\n",
    "    print(\"üìù Please run previous cells first\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki created successfully with ID: 636540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'636540'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 11: CREATE DATASET WIKI\n",
    "def set_wiki_content(wiki_content,dataset_id) -> str: \n",
    "    wiki = syn.store(Wiki(title=\"Dataset Documentation\", markdown=wiki_content, owner=dataset_id))\n",
    "    print(f\"Wiki created successfully with ID: {wiki.id}\")\n",
    "    return wiki.id\n",
    "wiki_content=\"\"\"\n",
    "**Summary:** The study describes the first large-scale, NIH-funded multicenter Expanded Access Protocol (EAP) for ALS in the United States, offering investigational intravenous trehalose (SLS-005) to individuals with ALS who were ineligible for concurrent randomized clinical trials (RCTs). Over 24 weeks, 70 participants enrolled at 20 sites received trehalose infusions and provided detailed clinical and biomarker data. No significant benefit was found for neurofilament light levels, functional status, or survival compared to historical controls. Biomarker and clinical data, as well as additional banked serum samples, are now available to the research community for further study.\n",
    "\n",
    "**Overall Design:**\n",
    "- Multicenter, open-label, non-randomized EAP for ALS patients ineligible for RCTs.\n",
    "- Conducted at 20 sites across the US, involving up to 24 weeks of weekly intravenous trehalose infusion.\n",
    "- Two participant cohorts: (1) ALS patients na√Øve to trehalose and trial-ineligible and (2) previous trial participants needing continued drug access.\n",
    "- Outcomes included safety/tolerability, neurofilament light chain (NfL) biomarker, ALS Functional Rating Scale-Revised (ALSFRS-R), slow vital capacity, survival, and quality of life.\n",
    "- Data and serum samples shared in public repositories for further research; protocol included patient and caregiver involvement, and remote/home infusions when possible.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Show More</summary>\n",
    "<b>Contact:</b>\n",
    "\n",
    "- Sabrina Paganoni, MD, PhD; spaganoni@mgh.harvard.edu.\n",
    "- Sean M. Healey & AMG Center for ALS and Neurological Clinical Research Institute, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA  \n",
    "\n",
    "<b>Contributors:</b>  Senda Ajroud-Driss, Suma Babu, James D. Berry, Cynthia Bodkin, Namita A. Goyal, Kelly Gwathmey, Daragh Heitzman, Shafeeq Ladha, Courtney E. McIlduff, Sabrina Paganoni, Laura Rosow, Mr. Alexander V. Sherman, David Walk, Jackie Whitesell, Eufrosina Young, Warren Wasiewski\n",
    "\n",
    "<b>Publication:</b>\n",
    "\n",
    "- Krivickas B, Scirocco E, Giacomelli E, Sharma S, et al. Multicenter Expanded Access Protocol for Research Through Access to Trehalose in People With Amyotrophic Lateral Sclerosis. *Muscle & Nerve*. 2025;0:1‚Äì9. doi: 10.1002/mus.70011.\n",
    "[**DOI:** 10.1002/mus.70011](https://doi.org/10.1002/mus.70011)\n",
    "\n",
    "</details>\n",
    "\"\"\"\n",
    "set_wiki_content(wiki_content,\"syn72016774\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "828249a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn72016774:Trehalose Biomarker Dataset]: (Column Order): ['id', 'name', 'fileFormat', 'studyType', 'dataType', 'disease', 'description', 'createdOn', 'createdBy', 'etag', 'modifiedOn', 'modifiedBy', 'path', 'type', 'currentVersion', 'parentId', 'benefactorId', 'projectId', 'dataFileHandleId', 'dataFileName', 'dataFileSizeBytes', 'dataFileMD5Hex', 'dataFileConcreteType', 'dataFileBucket', 'dataFileKey', 'species', 'dataFormat', 'assay', 'platform', 'libraryStrategy', 'libraryLayout', 'cellType', 'biospecimenType', 'processingLevel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/entity/syn72016774/table/transaction/async: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00/1.00 [00:01<00:00, 1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# CELL 12: DATASET COLUMN REORDERING \n",
    "def reorder_dataset_columns(syn, dataset_id, desired_column_order):\n",
    "    dataset=Dataset(id=dataset_id).get(include_columns=True)\n",
    "    current_columns = list(dataset.columns.keys())\n",
    "    final_order=[]\n",
    "    for col in desired_column_order:\n",
    "        if col in current_columns:\n",
    "            final_order.append(col)\n",
    "    remaining_cols = [col for col in current_columns if col not in final_order]\n",
    "    final_order.extend(remaining_cols)\n",
    "    for target_index, col_name in enumerate(final_order):\n",
    "        dataset.reorder_column(name=col_name, index=target_index)\n",
    "    dataset.store()\n",
    "\n",
    "# Template-based column ordering (adapted for SRA dataset)\n",
    "# Based on syn68808453 but modified for SRA-specific fields\n",
    "template_column_order = [\n",
    "        # === SYSTEM COLUMNS (keep at front) ===\n",
    "        'id',\n",
    "        'name',\n",
    "\n",
    "        # === KEY FILE ANNOTATIONS (high priority) ===\n",
    "        'fileFormat',\n",
    "        'studyType',\n",
    "        'dataType',\n",
    "        'keyMeasures',\n",
    "        'assessmentType',\n",
    "        'ClinicalDomain',\n",
    "        'hasLongitudinalData',\n",
    "        'disease',\n",
    "        # === DEFAULT SYNAPSE COLUMNS (maintain original order) ===\n",
    "        'description',\n",
    "        'createdOn',\n",
    "        'createdBy',\n",
    "        'etag',\n",
    "        'modifiedOn',\n",
    "        'modifiedBy',\n",
    "        'path',\n",
    "        'type',\n",
    "        'currentVersion',\n",
    "        'parentId',\n",
    "        'benefactorId',\n",
    "        'projectId',\n",
    "        'dataFileHandleId',\n",
    "        'dataFileName',\n",
    "        'dataFileSizeBytes',\n",
    "        'dataFileMD5Hex',\n",
    "        'dataFileConcreteType',\n",
    "        'dataFileBucket',\n",
    "        'dataFileKey'\n",
    "]\n",
    "reorder_dataset_columns(syn, \"syn72016774\", template_column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3fd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 15: REORDER DATASET COLUMNS\n",
    "\n",
    "from synapseclient.models import Dataset\n",
    "\n",
    "def get_column_order_template(dataset_type):\n",
    "    \"\"\"Get column order template based on dataset type.\"\"\"\n",
    "    \n",
    "    # System columns - always first\n",
    "    system_columns = [\n",
    "        'id',\n",
    "        'name',\n",
    "    ]\n",
    "    \n",
    "    # Shared high-priority annotation columns\n",
    "    shared_priority = [\n",
    "        'dataType',\n",
    "        'fileFormat',\n",
    "        'studyType',\n",
    "        'species',\n",
    "        'disease',\n",
    "        'dataFormat',\n",
    "    ]\n",
    "    \n",
    "    # Clinical-specific priority columns\n",
    "    clinical_priority = [\n",
    "        'studyPhase',\n",
    "        'assessmentType',\n",
    "        'clinicalDomain',\n",
    "        'keyMeasures',\n",
    "        'hasLongitudinalData',\n",
    "        'studyDesign',\n",
    "        'primaryOutcome',\n",
    "    ]\n",
    "    \n",
    "    # Omic-specific priority columns\n",
    "    omic_priority = [\n",
    "        'assay',\n",
    "        'platform',\n",
    "        'libraryStrategy',\n",
    "        'libraryLayout',\n",
    "        'cellType',\n",
    "        'biospecimenType',\n",
    "        'processingLevel',\n",
    "    ]\n",
    "    \n",
    "    # Standard Synapse columns - keep after annotations\n",
    "    synapse_columns = [\n",
    "        'description',\n",
    "        'createdOn',\n",
    "        'createdBy',\n",
    "        'etag',\n",
    "        'modifiedOn',\n",
    "        'modifiedBy',\n",
    "        'path',\n",
    "        'type',\n",
    "        'currentVersion',\n",
    "        'parentId',\n",
    "        'benefactorId',\n",
    "        'projectId',\n",
    "        'dataFileHandleId',\n",
    "        'dataFileName',\n",
    "        'dataFileSizeBytes',\n",
    "        'dataFileMD5Hex',\n",
    "        'dataFileConcreteType',\n",
    "        'dataFileBucket',\n",
    "        'dataFileKey',\n",
    "    ]\n",
    "    \n",
    "    # Build complete order based on dataset type\n",
    "    if dataset_type and 'omic' in dataset_type.lower():\n",
    "        return system_columns + shared_priority + omic_priority + synapse_columns\n",
    "    else:\n",
    "        return system_columns + shared_priority + clinical_priority + synapse_columns\n",
    "\n",
    "def reorder_dataset_columns(syn, dataset_id, dataset_type=None):\n",
    "    \"\"\"Reorder dataset columns based on template.\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Reordering dataset columns...\")\n",
    "        print(f\"   üìã Dataset ID: {dataset_id}\")\n",
    "        print(f\"   üìä Dataset Type: {dataset_type or 'Auto-detect'}\")\n",
    "        \n",
    "        # Get the dataset with columns\n",
    "        print(f\"   üîÑ Loading dataset with columns...\")\n",
    "        dataset = Dataset(id=dataset_id).get(include_columns=True)\n",
    "        \n",
    "        # Auto-detect dataset type if not provided\n",
    "        if not dataset_type:\n",
    "            annotations = dataset.annotations if hasattr(dataset, 'annotations') else {}\n",
    "            dataset_type = annotations.get('_dataset_type', 'ClinicalDataset')\n",
    "            print(f\"   üîç Auto-detected type: {dataset_type}\")\n",
    "        \n",
    "        # Get column order template\n",
    "        template_order = get_column_order_template(dataset_type)\n",
    "        \n",
    "        # Get current columns\n",
    "        current_columns = list(dataset.columns.keys())\n",
    "        print(f\"   üìä Current columns: {len(current_columns)}\")\n",
    "        print(f\"   üìê Template positions: {len(template_order)}\")\n",
    "        \n",
    "        # Build final order: template columns first, then any remaining\n",
    "        final_order = []\n",
    "        \n",
    "        # Add columns from template (if they exist)\n",
    "        for col in template_order:\n",
    "            if col in current_columns:\n",
    "                final_order.append(col)\n",
    "        \n",
    "        # Add any remaining columns not in template\n",
    "        remaining_columns = [col for col in current_columns if col not in final_order]\n",
    "        final_order.extend(remaining_columns)\n",
    "        \n",
    "        print(f\"\\nüìä Reordering plan:\")\n",
    "        print(f\"   Template-ordered: {len(final_order) - len(remaining_columns)}\")\n",
    "        print(f\"   Additional columns: {len(remaining_columns)}\")\n",
    "        if remaining_columns:\n",
    "            preview = ', '.join(remaining_columns[:5])\n",
    "            if len(remaining_columns) > 5:\n",
    "                preview += f\" + {len(remaining_columns) - 5} more\"\n",
    "            print(f\"   Additional: {preview}\")\n",
    "        \n",
    "        # Execute reordering\n",
    "        print(f\"\\nüîÑ Executing column reordering...\")\n",
    "        reorder_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for target_index, col_name in enumerate(final_order):\n",
    "            try:\n",
    "                current_position = list(dataset.columns.keys()).index(col_name)\n",
    "                \n",
    "                if current_position != target_index:\n",
    "                    dataset.reorder_column(name=col_name, index=target_index)\n",
    "                    # Show details for first 15 moves\n",
    "                    if reorder_count < 15:\n",
    "                        print(f\"   üîÑ '{col_name}': position {current_position} ‚Üí {target_index}\")\n",
    "                    reorder_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to reorder '{col_name}': {e}\")\n",
    "                failed_count += 1\n",
    "        \n",
    "        if reorder_count > 15:\n",
    "            print(f\"   ... and {reorder_count - 15} more reorderings\")\n",
    "        \n",
    "        # Store the dataset if changes were made\n",
    "        if reorder_count > 0:\n",
    "            print(f\"\\nüíæ Storing dataset with new column order...\")\n",
    "            updated_dataset = dataset.store()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Successfully reordered columns!\")\n",
    "            print(f\"   ‚úì Reordered: {reorder_count} columns\")\n",
    "            if failed_count > 0:\n",
    "                print(f\"   ‚úó Failed: {failed_count} columns\")\n",
    "            \n",
    "            # Show new column order (first 20)\n",
    "            final_columns = list(updated_dataset.columns.keys())\n",
    "            print(f\"\\nüìã NEW COLUMN ORDER (first 20 of {len(final_columns)}):\")\n",
    "            \n",
    "            for i, col in enumerate(final_columns[:20]):\n",
    "                # Mark template vs additional columns\n",
    "                marker = \"üéØ\" if col in template_order else \"‚ûï\"\n",
    "                print(f\"   {i+1:2d}. {marker} {col}\")\n",
    "            \n",
    "            if len(final_columns) > 20:\n",
    "                print(f\"   ... and {len(final_columns) - 20} more columns\")\n",
    "            \n",
    "            # Template compliance\n",
    "            template_cols_in_front = sum(1 for col in final_columns[:len(template_order)] if col in template_order)\n",
    "            print(f\"\\nüéØ Template Compliance:\")\n",
    "            print(f\"   Template columns positioned correctly: {template_cols_in_front}\")\n",
    "            \n",
    "            print(f\"\\nüîó Dataset URL: https://www.synapse.org/#!Synapse:{dataset_id}\")\n",
    "            \n",
    "            return updated_dataset\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Columns already in correct order!\")\n",
    "            return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reordering columns: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute: Reorder dataset columns\n",
    "print(\"üèóÔ∏è  REORDER DATASET COLUMNS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Re-establish variables if needed\n",
    "try:\n",
    "    if 'syn' not in globals() or syn is None:\n",
    "        print(\"‚ö†Ô∏è Reconnecting to Synapse...\")\n",
    "        syn = connect_to_synapse()\n",
    "    \n",
    "    # Check if we have a dataset_id from previous cells\n",
    "    if 'dataset_id' not in globals() or dataset_id is None:\n",
    "        print(\"‚ùå No dataset_id found from previous cells\")\n",
    "        print(\"üìù Please run Cell 12 (Dataset Entity Creation) first\")\n",
    "    else:\n",
    "        print(f\"üìã Using dataset from previous cell: {dataset_id}\")\n",
    "        \n",
    "        # Load dataset annotations to get type\n",
    "        if 'dataset_annotation_file_path' in globals() and os.path.exists(dataset_annotation_file_path):\n",
    "            dataset_annotations = load_existing_annotations(dataset_annotation_file_path)\n",
    "            dataset_type = dataset_annotations.get('_dataset_type', 'ClinicalDataset')\n",
    "        else:\n",
    "            dataset_type = None  # Will auto-detect\n",
    "        \n",
    "        # Reorder columns\n",
    "        updated_dataset = reorder_dataset_columns(\n",
    "            syn, \n",
    "            dataset_id,\n",
    "            dataset_type=dataset_type\n",
    "        )\n",
    "        \n",
    "        if updated_dataset:\n",
    "            print(f\"\\nüéØ BENEFITS OF COLUMN ORDERING:\")\n",
    "            print(f\"   üìã Consistent layout across datasets\")\n",
    "            print(f\"   üë• Familiar structure for researchers\")\n",
    "            print(f\"   üîç Key annotations prominently positioned\")\n",
    "            print(f\"   üìä Technical metadata logically grouped\")\n",
    "        else:\n",
    "            print(\"‚ùå Could not reorder columns\")\n",
    "            \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Missing configuration: {e}\")\n",
    "    print(\"üìù Please run previous cells first\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb77a6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': ['https://www.synapse.org/Synapse:syn68927891'],\n",
       " 'assay': ['ELISA',\n",
       "  'quantitative PCR',\n",
       "  'miRNA-seq',\n",
       "  'liquid chromatography/tandem mass spectrometry',\n",
       "  'proximity extension assay',\n",
       "  'immunoassay',\n",
       "  'Protein target assay'],\n",
       " 'title': ['Trehalose Biomarker Dataset'],\n",
       " 'sameAs': [''],\n",
       " 'source': ['ALL ALS Clinical Trials'],\n",
       " 'creator': ['Sabrina Paganoni, MD, PhD'],\n",
       " 'disease': [''],\n",
       " 'license': [''],\n",
       " 'species': ['Homo sapiens'],\n",
       " 'dataType': ['clinical',\n",
       "  'biomarker',\n",
       "  'metabolomics',\n",
       "  'proteomics',\n",
       "  'SomaScan'],\n",
       " 'keywords': ['Amyotrophic Lateral Sclerosis',\n",
       "  'trehalose',\n",
       "  'biomarker',\n",
       "  'metabolomics',\n",
       "  'proteomics',\n",
       "  'SomaScan'],\n",
       " 'platform': [''],\n",
       " 'publisher': [''],\n",
       " 'dataFormat': ['excel', 'csv', 'pdf'],\n",
       " 'contributor': ['Sabrina Paganoni, MD, PhD'],\n",
       " 'description': [''],\n",
       " 'alternateName': [''],\n",
       " 'curationLevel': [''],\n",
       " 'datePublished': [''],\n",
       " 'FACSPopulation': [''],\n",
       " 'GEOSuperSeries': [''],\n",
       " 'individualCount': ['70'],\n",
       " 'libraryStrategy': [''],\n",
       " 'processingLevel': [''],\n",
       " 'originalSampleName': [''],\n",
       " 'includedInDataCatalog': [''],\n",
       " 'acknowledgementStatement': 'syn64892175/wiki/633969'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 13: Acknowledgement Statement & description\n",
    "def set_acknowledgement_statement(syn, dataset_id, statement, description=\"\"):\n",
    "    dataset=syn.get(dataset_id, downloadFile=False)\n",
    "    dataset_annotations=dataset.annotations\n",
    "    dataset_annotations['acknowledgementStatement'] = statement\n",
    "    dataset.annotations = dataset_annotations\n",
    "    dataset.description = description\n",
    "    syn.store(dataset, forceVersion=False)\n",
    "    return dataset_annotations\n",
    "set_acknowledgement_statement(syn, \"syn72016774\", \"syn64892175/wiki/633969\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbad197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: SET DATASET PERMISSIONS\n",
    "def set_dataset_permissions(syn, dataset_id, principal_id, access_type, modify_benefactor=False, overwrite=True):\n",
    "    dataset = Dataset(id=dataset_id).get(include_columns=True)\n",
    "    dataset.get_permissions()\n",
    "    main_folder_permissions = [\"READ\", \"DOWNLOAD\"]\n",
    "    dataset.set_permissions(\n",
    "        principal_id=principal_id,\n",
    "        access_type=main_folder_permissions,\n",
    "        modify_benefactor=modify_benefactor,  # Create local ACL for this folder\n",
    "        overwrite=overwrite,\n",
    "    )\n",
    "set_dataset_permissions(syn, \"syn72016774\", 273948, [\"READ\", \"DOWNLOAD\"], modify_benefactor=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5698b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn72016774:Trehalose Biomarker Dataset]: Creating a snapshot of the <class 'synapseclient.models.dataset.Dataset'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/entity/syn72016774/table/transaction/async: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00/1.00 [00:01<00:00, 1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created snapshot successfully!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TableUpdateTransaction' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   üîó URL: https://www.synapse.org/#!Synapse:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m snapshot\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m---> 10\u001b[0m \u001b[43msnapshot_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msyn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msyn72016774\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrehalose Biomarker 2026.1 release\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2026.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m, in \u001b[0;36msnapshot_dataset\u001b[0;34m(syn, dataset_id, comment, label)\u001b[0m\n\u001b[1;32m      4\u001b[0m snapshot \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msnapshot(comment\u001b[38;5;241m=\u001b[39mcomment, label\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Created snapshot successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   üìã Snapshot ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   üè∑Ô∏è Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   üîó URL: https://www.synapse.org/#!Synapse:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TableUpdateTransaction' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "# CELL 14: SNAPSHOT\n",
    "def snapshot_dataset(syn, dataset_id, comment=\"Dataset snapshot\", label=\"v1.0\"):\n",
    "    dataset = Dataset(id=dataset_id).get(include_columns=True)\n",
    "    snapshot = dataset.snapshot(comment=comment, label=label)\n",
    "    print(f\"‚úÖ Created snapshot successfully!\")\n",
    "    #print(f\"   üìã Snapshot ID: {snapshot.id}\")\n",
    "    print(f\"   üè∑Ô∏è Name: {snapshot.name}\")\n",
    "    print(f\"   üîó URL: https://www.synapse.org/#!Synapse:{snapshot.id}\")\n",
    "    return snapshot.id\n",
    "snapshot_dataset(syn, \"syn72016774\", comment=\"Trehalose Biomarker 2026.1 release\", label=\"2026.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdb0de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] Error occurred while running store_async on <class 'synapseclient.models.dataset.DatasetCollection'>.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/core/async_utils.py\", line 133, in newmethod\n",
      "    return loop.run_until_complete(wrapper(*args, **kwargs))\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/core/async_utils.py\", line 122, in wrapper\n",
      "    return await getattr(self, async_method_name)(*args, **kwargs)\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/models/dataset.py\", line 2498, in store_async\n",
      "    return await super().store_async(\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/models/mixins/table_components.py\", line 599, in store_async\n",
      "    return await super().store_async(\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/core/async_utils.py\", line 56, in otel_trace_method_wrapper\n",
      "    return await func(self, *arg, **kwargs)\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/models/mixins/table_components.py\", line 446, in store_async\n",
      "    entity = await put_entity_id_bundle2(\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/api/entity_bundle_services_v2.py\", line 150, in put_entity_id_bundle2\n",
      "    return await client.rest_put_async(\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/client.py\", line 6616, in rest_put_async\n",
      "    response = await self._rest_call_async(\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/client.py\", line 6506, in _rest_call_async\n",
      "    self._handle_httpx_synapse_http_error(response)\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/client.py\", line 6191, in _handle_httpx_synapse_http_error\n",
      "    exceptions._raise_for_status_httpx(\n",
      "  File \"/home/ramayyala/.local/share/mamba/envs/amp-als/lib/python3.10/site-packages/synapseclient/core/exceptions.py\", line 296, in _raise_for_status_httpx\n",
      "    raise SynapseHTTPError(message, response=response)\n",
      "synapseclient.core.exceptions.SynapseHTTPError: 400 Client Error: Each dataset collection item must have a unique entity ID.  Duplicate: syn72016774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error adding Dataset to Collection: 400 Client Error: Each dataset collection item must have a unique entity ID.  Duplicate: syn72016774\n"
     ]
    }
   ],
   "source": [
    "# CELL 15: Add Dataset to Prod Collection \n",
    "def add_dataset_to_collection(syn, dataset_id, collection_id):\n",
    "    try:\n",
    "\n",
    "        dataset_collection = DatasetCollection(id=collection_id).get()\n",
    "        dataset = Dataset(id=dataset_id).get(include_columns=True)\n",
    "        dataset_collection.add_item(dataset)\n",
    "        dataset_collection.store()\n",
    "        print(f\"‚úÖ Added Dataset {dataset_id} to Collection {collection_id} successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding Dataset to Collection: {e}\")\n",
    "add_dataset_to_collection(syn, \"syn72016774\", DATASETS_COLLECTION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34be7211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved folder to: syn68885183\n"
     ]
    }
   ],
   "source": [
    "# CELL 15: Move to release\n",
    "def move_folder(syn, folder_id, new_parent_id):\n",
    "    folder = Folder(id=folder_id).get()\n",
    "    folder.parent_id = new_parent_id\n",
    "    folder = folder.store()\n",
    "    print(f\"Moved folder to: {folder.parent_id}\")\n",
    "move_folder(syn, STAGING_FOLDER_ID, RELEASE_FOLDER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amp-als",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
