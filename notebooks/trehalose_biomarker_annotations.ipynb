{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Trehalose Biomarker Data Annotations Workflow\n",
    "\n",
    "**Complete workflow for managing file annotations using the data model**\n",
    "\n",
    "## Workflow Steps:\n",
    "1. **Libraries & Configuration** - Load dependencies and configure paths\n",
    "2. **File Enumeration** - Connect to Synapse and enumerate files/folders\n",
    "3. **Annotation Management** - Create/update annotation templates dynamically\n",
    "4. **Validation** - Validate annotations against data model schemas\n",
    "5. **Application** - Apply validated annotations to Synapse entities\n",
    "\n",
    "## Key Features:\n",
    "- ğŸ”„ **Dynamic annotation management**: Only adds new files, preserves existing annotations\n",
    "- ğŸ“‹ **Schema-driven validation**: Validates against ClinicalFile and OmicFile schemas\n",
    "- ğŸ¯ **Smart file type detection**: Automatically determines Clinical vs Omic data\n",
    "- ğŸ’¾ **Persistent storage**: Saves annotations to `./annotations/{folder_name}.json`\n",
    "- âœ… **Complete validation**: Blocks progression until all annotations are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Libraries loaded successfully\n",
      "ğŸ Python version: 2.3.2 (pandas)\n",
      "ğŸ”— Synapse client version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: LIBRARIES & IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import synapseclient\n",
    "from synapseclient.models import (\n",
    "    Column, ColumnType, Dataset, EntityRef, File, Folder, Project, FacetType, DatasetCollection\n",
    ")\n",
    "from typing import Dict, List, Any, Set, Union\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š Libraries loaded successfully\")\n",
    "print(f\"ğŸ Python version: {pd.__version__} (pandas)\")\n",
    "print(f\"ğŸ”— Synapse client version: {synapseclient.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  CONFIGURATION LOADED\n",
      "========================================\n",
      "ğŸ“ Staging Folder: syn68927891\n",
      "ğŸ—ï¸  Project: syn68702804\n",
      "ğŸ“„ Data Model Path: ../model_schemas\n",
      "ğŸ’¾ Annotations Directory: ./annotations\n",
      "ğŸ” Dry Run Mode: False\n",
      "ğŸ“‚ Annotations directory ready: ./annotations\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: CONFIGURATION SETUP\n",
    "\n",
    "# Synapse Configuration\n",
    "STAGING_FOLDER_ID = \"syn68927891\"  # Trehalose Biomarker Data folder\n",
    "PROJECT_ID = \"syn68702804\"\n",
    "RELEASE_FOLDER_ID = \"syn68885183\"\n",
    "DATASETS_COLLECTION_ID = \"syn66496326\"\n",
    "DRY_RUN = False  # Set to False to actually apply changes\n",
    "\n",
    "# Data Model Configuration\n",
    "DATA_MODEL_PATH = \"../model_schemas\"  # Path to YAML schema directory\n",
    "DATA_MODEL_FILE = \"../dist/ALS.yaml\"  # Main compiled data model (optional)\n",
    "\n",
    "# Annotation Management Configuration\n",
    "ANNOTATIONS_DIR = \"./annotations\"  # Local directory for annotation files\n",
    "\n",
    "# Authentication Token (replace with your token or use .synapseConfig)\n",
    "\n",
    "print(\"âš™ï¸  CONFIGURATION LOADED\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ğŸ“ Staging Folder: {STAGING_FOLDER_ID}\")\n",
    "print(f\"ğŸ—ï¸  Project: {PROJECT_ID}\")\n",
    "print(f\"ğŸ“„ Data Model Path: {DATA_MODEL_PATH}\")\n",
    "print(f\"ğŸ’¾ Annotations Directory: {ANNOTATIONS_DIR}\")\n",
    "print(f\"ğŸ” Dry Run Mode: {DRY_RUN}\")\n",
    "\n",
    "# Create annotations directory if it doesn't exist\n",
    "os.makedirs(ANNOTATIONS_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“‚ Annotations directory ready: {ANNOTATIONS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "file_enumeration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— SYNAPSE CONNECTION & FILE ENUMERATION\n",
      "=============================================\n",
      "\n",
      "UPGRADE AVAILABLE\n",
      "\n",
      "A more recent version of the Synapse Client (4.10.0) is available. Your version (4.8.0) can be upgraded by typing:\n",
      "   pip install --upgrade synapseclient\n",
      "\n",
      "Python Synapse Client version 4.10.0 release notes\n",
      "\n",
      "https://python-docs.synapse.org/news/\n",
      "\n",
      "\n",
      "\n",
      "UPGRADE AVAILABLE\n",
      "\n",
      "A more recent version of the Synapse Client (4.10.0) is available. Your version (4.8.0) can be upgraded by typing:\n",
      "   pip install --upgrade synapseclient\n",
      "\n",
      "Python Synapse Client version 4.10.0 release notes\n",
      "\n",
      "https://python-docs.synapse.org/news/\n",
      "\n",
      "\n",
      "Welcome, ram.ayyala!\n",
      "\n",
      "âœ… Connected to Synapse\n",
      "ğŸ” Starting enumeration of folder syn68927891...\n",
      "Welcome, ram.ayyala!\n",
      "\n",
      "âœ… Connected to Synapse\n",
      "ğŸ” Starting enumeration of folder syn68927891...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927891:Trehalose Biomarker Data]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927979:Answer Clinical Data]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927983:Protavio]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927982:Metabolomics]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927980:ICON]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[syn68927984:Somalogic]: Syncing Folder from Synapse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syncing from Synapse:   0%|          | 0.00/1.00 [00:01<?, ?B/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“ Answer Clinical Data (syn68927979)\n",
      "   ğŸ” Processing subfolder: Answer Clinical Data\n",
      "   ğŸ“„ Answer Clinical Data/2024.T.15_Answer_ClinicalData_External.xlsx (syn68929229)\n",
      "   ğŸ“ ICON (syn68927980)\n",
      "   ğŸ” Processing subfolder: ICON\n",
      "   ğŸ“„ ICON/0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx (syn68929235)\n",
      "   ğŸ“„ ICON/0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx (syn68929236)\n",
      "   ğŸ“„ ICON/0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx (syn68929234)\n",
      "   ğŸ“„ ICON/0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx (syn68929236)\n",
      "   ğŸ“„ ICON/0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx (syn68929234)\n",
      "   ğŸ“„ ICON/Total File_0869-0004-A.xlsx (syn68929233)\n",
      "   ğŸ“ Metabolomics (syn68927982)\n",
      "   ğŸ” Processing subfolder: Metabolomics\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_C18-neg.xlsx (syn68929241)\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_C8-pos.xlsx (syn68929242)\n",
      "   ğŸ“„ ICON/Total File_0869-0004-A.xlsx (syn68929233)\n",
      "   ğŸ“ Metabolomics (syn68927982)\n",
      "   ğŸ” Processing subfolder: Metabolomics\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_C18-neg.xlsx (syn68929241)\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_C8-pos.xlsx (syn68929242)\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_HILIC-neg.xlsx (syn68929239)\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_HILIC-pos.xlsx (syn68929240)\n",
      "   ğŸ“ Protavio (syn68927983)\n",
      "   ğŸ” Processing subfolder: Protavio\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_HILIC-neg.xlsx (syn68929239)\n",
      "   ğŸ“„ Metabolomics/25_0805_Trehalose_EAP_HILIC-pos.xlsx (syn68929240)\n",
      "   ğŸ“ Protavio (syn68927983)\n",
      "   ğŸ” Processing subfolder: Protavio\n",
      "   ğŸ“„ Protavio/PR-147-TES_RESULTS REPORT - Extended NPX.csv (syn68929253)\n",
      "   ğŸ“„ Protavio/PR-147-TES_STUDY REPORT_v1.0.pdf (syn68929249)\n",
      "   ğŸ“„ Protavio/PR-147-TES_RESULTS REPORT - Extended NPX.csv (syn68929253)\n",
      "   ğŸ“„ Protavio/PR-147-TES_STUDY REPORT_v1.0.pdf (syn68929249)\n",
      "   ğŸ“„ Protavio/PR-147-TES_STUDY REPORT_v2.0.pdf (syn68929248)\n",
      "   ğŸ“„ Protavio/correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx (syn68929247)\n",
      "   ğŸ“ Somalogic (syn68927984)\n",
      "   ğŸ” Processing subfolder: Somalogic\n",
      "   ğŸ“„ Protavio/PR-147-TES_STUDY REPORT_v2.0.pdf (syn68929248)\n",
      "   ğŸ“„ Protavio/correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx (syn68929247)\n",
      "   ğŸ“ Somalogic (syn68927984)\n",
      "   ğŸ” Processing subfolder: Somalogic\n",
      "   ğŸ“„ Somalogic/SS-25103982_SQS.pdf (syn68929255)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat (syn68929256)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat (syn68929257)\n",
      "   ğŸ“„ Somalogic/SS-25103982_SQS.pdf (syn68929255)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat (syn68929256)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat (syn68929257)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx (syn68929259)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx (syn68929258)\n",
      "ğŸ“Š Found 18 files and 5 folders\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx (syn68929259)\n",
      "   ğŸ“„ Somalogic/SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx (syn68929258)\n",
      "ğŸ“Š Found 18 files and 5 folders\n",
      "\n",
      "ğŸ“‹ Enumeration Complete:\n",
      "   ğŸ“ Folder: Trehalose Biomarker Data\n",
      "   ğŸ“Š Found: 23 files/folders\n",
      "   ğŸ’¾ Annotation file: ./annotations/trehalose_biomarker_data_annotations.json\n",
      "\n",
      "ğŸ“‹ Enumeration Complete:\n",
      "   ğŸ“ Folder: Trehalose Biomarker Data\n",
      "   ğŸ“Š Found: 23 files/folders\n",
      "   ğŸ’¾ Annotation file: ./annotations/trehalose_biomarker_data_annotations.json\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: FILE ENUMERATION AND ANNOTATION CREATION\n",
    "\n",
    "def connect_to_synapse():\n",
    "    \"\"\"Connect to Synapse.\"\"\"\n",
    "    try:\n",
    "        syn = synapseclient.Synapse()\n",
    "        syn.login(authToken=SYNAPSE_AUTH_TOKEN)\n",
    "        print(\"âœ… Connected to Synapse\")\n",
    "        return syn\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to connect: {e}\")\n",
    "        return None\n",
    "\n",
    "def enumerate_files_with_folders(syn, folder_id, include_folders=True, recursive=True):\n",
    "    \"\"\"Enumerate files and folders in a Synapse folder using modern API.\"\"\"\n",
    "    if not syn:\n",
    "        return {}\n",
    "    \n",
    "    items = {}\n",
    "    \n",
    "    def _process_folder(folder_obj, path_prefix=\"\"):\n",
    "        \"\"\"Process a folder object and its contents.\"\"\"\n",
    "        # Process files in this folder\n",
    "        if folder_obj.files:\n",
    "            for file in folder_obj.files:\n",
    "                try:\n",
    "                    file_entity = syn.get(file.id, downloadFile=False)\n",
    "                    file_name = file_entity.name if hasattr(file_entity, 'name') else file.name\n",
    "                    \n",
    "                    # Extract base name (remove extension)\n",
    "                    base_name = file_name\n",
    "                    for ext in ['.csv', '.txt', '.json', '.xml', '.tsv', '.xlsx', '.pdf', '.docx', '.html', '.md', '.adat']:\n",
    "                        if file_name.lower().endswith(ext.lower()):\n",
    "                            base_name = file_name[:-(len(ext))]\n",
    "                            break\n",
    "                    \n",
    "                    current_path = f\"{path_prefix}/{file_name}\" if path_prefix else file_name\n",
    "                    items[file.id] = {\n",
    "                        'name': file_name,\n",
    "                        'base_name': base_name,\n",
    "                        'id': file.id,\n",
    "                        'type': 'file',\n",
    "                        'path': current_path\n",
    "                    }\n",
    "                    print(f\"   ğŸ“„ {current_path} ({file.id})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Error getting file {file.id}: {e}\")\n",
    "        \n",
    "        # Process subfolders\n",
    "        if folder_obj.folders:\n",
    "            for subfolder in folder_obj.folders:\n",
    "                current_path = f\"{path_prefix}/{subfolder.name}\" if path_prefix else subfolder.name\n",
    "                \n",
    "                # Add folder metadata if requested\n",
    "                if include_folders:\n",
    "                    items[subfolder.id] = {\n",
    "                        'name': subfolder.name,\n",
    "                        'id': subfolder.id,\n",
    "                        'type': 'folder',\n",
    "                        'path': current_path\n",
    "                    }\n",
    "                    print(f\"   ğŸ“ {current_path} ({subfolder.id})\")\n",
    "                \n",
    "                # Recursively process subfolder if enabled\n",
    "                if recursive:\n",
    "                    print(f\"   ğŸ” Processing subfolder: {current_path}\")\n",
    "                    _process_folder(subfolder, current_path)\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ” Starting enumeration of folder {folder_id}...\")\n",
    "        \n",
    "        # Get the folder and sync from Synapse\n",
    "        folder = Folder(id=folder_id)\n",
    "        folder = folder.sync_from_synapse(download_file=False, recursive=recursive)\n",
    "        \n",
    "        # Process the folder and all its contents\n",
    "        _process_folder(folder)\n",
    "        \n",
    "        file_count = sum(1 for item in items.values() if item['type'] == 'file')\n",
    "        folder_count = sum(1 for item in items.values() if item['type'] == 'folder')\n",
    "        \n",
    "        print(f\"ğŸ“Š Found {file_count} files and {folder_count} folders\")\n",
    "        return items\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error enumerating files: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def get_staging_folder_name(syn, folder_id):\n",
    "    \"\"\"Extract folder name for annotation file naming.\"\"\"\n",
    "    try:\n",
    "        folder = syn.get(folder_id, downloadFile=False)\n",
    "        return folder.name\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting folder name: {e}\")\n",
    "        return \"unknown_folder\"\n",
    "\n",
    "def create_annotations_file_path(staging_folder_name):\n",
    "    \"\"\"Create path: ./annotations/{folder_name}_annotations.json\"\"\"\n",
    "    clean_name = staging_folder_name.lower().replace(' ', '_').replace('-', '_')\n",
    "    clean_name = re.sub(r'[^a-z0-9_]', '', clean_name)  # Remove special chars\n",
    "    return f\"{ANNOTATIONS_DIR}/{clean_name}_annotations.json\"\n",
    "\n",
    "def load_existing_annotations(file_path):\n",
    "    \"\"\"Load existing annotations if file exists.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_annotations(annotations, file_path):\n",
    "    \"\"\"Save annotations to JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(annotations, f, indent=2)\n",
    "\n",
    "# Execute file enumeration\n",
    "print(\"ğŸ”— SYNAPSE CONNECTION & FILE ENUMERATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "syn = connect_to_synapse()\n",
    "if syn:\n",
    "    files_folders = enumerate_files_with_folders(syn, STAGING_FOLDER_ID, include_folders=True, recursive=True)\n",
    "    staging_folder_name = get_staging_folder_name(syn, STAGING_FOLDER_ID)\n",
    "    annotation_file_path = create_annotations_file_path(staging_folder_name)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Enumeration Complete:\")\n",
    "    print(f\"   ğŸ“ Folder: {staging_folder_name}\")\n",
    "    print(f\"   ğŸ“Š Found: {len(files_folders)} files/folders\")\n",
    "    print(f\"   ğŸ’¾ Annotation file: {annotation_file_path}\")\n",
    "else:\n",
    "    print(\"âŒ Could not connect to Synapse - using simulated data\")\n",
    "    files_folders = {}\n",
    "    staging_folder_name = \"trehalose_biomarker_data\"\n",
    "    annotation_file_path = create_annotations_file_path(staging_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "schema_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” LOADING DATA MODEL SCHEMAS\n",
      "==============================\n",
      "Looking for schemas in: /home/ramayyala/Documents/data-model/model_schemas/**/*.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/base/BaseDataset.yaml\n",
      "  Found class: BaseDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/base/BaseFile.yaml\n",
      "  Found class: BaseFile\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/data-management.yaml\n",
      "  Found class: DataQuality\n",
      "  Found class: AssessmentAdministration\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/domains.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/genetic-profile.yaml\n",
      "  Found class: GeneticProfile\n",
      "  Found class: GeneticVariant\n",
      "  Found class: FamilyHistory\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/laboratory.yaml\n",
      "  Found class: LaboratoryCollection\n",
      "  Found class: LaboratoryResult\n",
      "  Found class: ChemistryPanel\n",
      "  Found class: Hematology\n",
      "  Found class: Biomarkers\n",
      "  Found class: Urinalysis\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/medical-history.yaml\n",
      "  Found class: MedicalHistoryCollection\n",
      "  Found class: MedicalHistory\n",
      "  Found class: Comorbidity\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/phenoconversion.yaml\n",
      "  Found class: Phenoconversion\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/study-management.yaml\n",
      "  Found class: StudyDisposition\n",
      "  Found class: Eligibility\n",
      "  Found class: ProtocolDeviation\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/treatments.yaml\n",
      "  Found class: Treatment\n",
      "  Found class: MedicalDevice\n",
      "  Found class: AdverseEvent\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/visits.yaml\n",
      "  Found class: Visit\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/dynamometry.yaml\n",
      "  Found class: MedicalHistoryCollection\n",
      "  Found class: MedicalHistory\n",
      "  Found class: Comorbidity\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/phenoconversion.yaml\n",
      "  Found class: Phenoconversion\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/study-management.yaml\n",
      "  Found class: StudyDisposition\n",
      "  Found class: Eligibility\n",
      "  Found class: ProtocolDeviation\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/treatments.yaml\n",
      "  Found class: Treatment\n",
      "  Found class: MedicalDevice\n",
      "  Found class: AdverseEvent\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/visits.yaml\n",
      "  Found class: Visit\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/dynamometry.yaml\n",
      "  Found class: HandHeldDynamometry\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/electrophysiology.yaml\n",
      "  Found class: ElectrophysiologyAssessment\n",
      "  Found class: NerveConductionStudy\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/neurological.yaml\n",
      "  Found class: NeurologicalAssessment\n",
      "  Found class: MotorAssessment\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/psychiatric.yaml\n",
      "  Found class: SuicideRiskAssessment\n",
      "  Found class: PsychiatricHistory\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/speech.yaml\n",
      "  Found class: DigitalSpeechAssessment\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/symptom-questionnaire.yaml\n",
      "  Found class: SymptomQuestionnaire\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/vital-signs-physical.yaml\n",
      "  Found class: HandHeldDynamometry\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/electrophysiology.yaml\n",
      "  Found class: ElectrophysiologyAssessment\n",
      "  Found class: NerveConductionStudy\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/neurological.yaml\n",
      "  Found class: NeurologicalAssessment\n",
      "  Found class: MotorAssessment\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/psychiatric.yaml\n",
      "  Found class: SuicideRiskAssessment\n",
      "  Found class: PsychiatricHistory\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/speech.yaml\n",
      "  Found class: DigitalSpeechAssessment\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/symptom-questionnaire.yaml\n",
      "  Found class: SymptomQuestionnaire\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/clinical/assessments/vital-signs-physical.yaml\n",
      "  Found class: VitalSigns\n",
      "  Found class: PhysicalExam\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/ClinicalDataset.yaml\n",
      "  Found class: ClinicalDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/ClinicalFile.yaml\n",
      "  Found class: ClinicalFile\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/OmicDataset.yaml\n",
      "  Found class: OmicDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/OmicFile.yaml\n",
      "  Found class: OmicFile\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/AllDatasets.yaml\n",
      "  Found class: BaseDataset\n",
      "  Found class: ClinicalDataset\n",
      "  Found class: OmicDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/Biospecimen.yaml\n",
      "  Found class: Biospecimen\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/ClinicalAssessment.yaml\n",
      "  Found class: ClinicalAssessment\n",
      "  Found class: ALSFRSAssessment\n",
      "  Found class: CognitiveAssessment\n",
      "  Found class: StrengthAssessment\n",
      "  Found class: PulmonaryFunction\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/Subject.yaml\n",
      "  Found class: Subject\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/governance/licenses.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/governance/portals.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/mixins/DatasetMixins.yaml\n",
      "  Found class: ClinicalDatasetMixin\n",
      "  Found class: OmicDatasetMixin\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/mixins/FileMixins.yaml\n",
      "  Found class: VitalSigns\n",
      "  Found class: PhysicalExam\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/ClinicalDataset.yaml\n",
      "  Found class: ClinicalDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/ClinicalFile.yaml\n",
      "  Found class: ClinicalFile\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/OmicDataset.yaml\n",
      "  Found class: OmicDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/datasets/OmicFile.yaml\n",
      "  Found class: OmicFile\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/AllDatasets.yaml\n",
      "  Found class: BaseDataset\n",
      "  Found class: ClinicalDataset\n",
      "  Found class: OmicDataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/Biospecimen.yaml\n",
      "  Found class: Biospecimen\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/ClinicalAssessment.yaml\n",
      "  Found class: ClinicalAssessment\n",
      "  Found class: ALSFRSAssessment\n",
      "  Found class: CognitiveAssessment\n",
      "  Found class: StrengthAssessment\n",
      "  Found class: PulmonaryFunction\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/entities/Subject.yaml\n",
      "  Found class: Subject\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/governance/licenses.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/governance/portals.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/mixins/DatasetMixins.yaml\n",
      "  Found class: ClinicalDatasetMixin\n",
      "  Found class: OmicDatasetMixin\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/mixins/FileMixins.yaml\n",
      "  Found class: ClinicalFileMixin\n",
      "  Found class: OmicFileMixin\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/assays.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/parameters.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/platforms.yaml\n",
      "  Found class: ClinicalFileMixin\n",
      "  Found class: OmicFileMixin\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/assays.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/parameters.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/omics/platforms.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/portal/Dataset.yaml\n",
      "  Found class: Dataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/portal/File.yaml\n",
      "  Found class: File\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/file-formats.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/sex.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/species.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/analysis-methods.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/annotations.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/common-enums.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/props.yaml\n",
      "Total classes loaded: 52\n",
      "\n",
      "âœ… Loaded 52 schema classes\n",
      "âœ… Found ClinicalFile\n",
      "âœ… Found OmicFile\n",
      "âœ… Found BaseFile\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "\n",
      "ğŸ“Š Schema Analysis:\n",
      "   ClinicalFile attributes: 29\n",
      "   OmicFile attributes: 33\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/portal/Dataset.yaml\n",
      "  Found class: Dataset\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/portal/File.yaml\n",
      "  Found class: File\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/data-types.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/file-formats.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/sex.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/reference/species.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/analysis-methods.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/annotations.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/common-enums.yaml\n",
      "Processing: /home/ramayyala/Documents/data-model/model_schemas/shared/props.yaml\n",
      "Total classes loaded: 52\n",
      "\n",
      "âœ… Loaded 52 schema classes\n",
      "âœ… Found ClinicalFile\n",
      "âœ… Found OmicFile\n",
      "âœ… Found BaseFile\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "\n",
      "ğŸ“Š Schema Analysis:\n",
      "   ClinicalFile attributes: 29\n",
      "   OmicFile attributes: 33\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: ENHANCED SCHEMA LOADING FUNCTIONS\n",
    "\n",
    "def get_all_schemas(schema_base_path=None):\n",
    "    \"\"\"Load all schema classes from YAML files in the model_schemas directory.\"\"\"\n",
    "    if schema_base_path is None:\n",
    "        # Get the notebook's directory and find model_schemas relative to it\n",
    "        notebook_dir = Path.cwd()\n",
    "        # Look for model_schemas in the current directory or parent directories\n",
    "        schema_base_path = None\n",
    "        for parent in [notebook_dir] + list(notebook_dir.parents):\n",
    "            potential_path = parent / 'model_schemas'\n",
    "            if potential_path.exists():\n",
    "                schema_base_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if schema_base_path is None:\n",
    "            # Fallback to relative path\n",
    "            schema_base_path = Path('model_schemas')\n",
    "    \n",
    "    schema_path = str(schema_base_path / '**' / '*.yaml')\n",
    "    all_schemas = {}\n",
    "    \n",
    "    print(f\"Looking for schemas in: {schema_path}\")\n",
    "    \n",
    "    for schema_file in glob.glob(schema_path, recursive=True):\n",
    "        print(f\"Processing: {schema_file}\")\n",
    "        with open(schema_file, 'r') as f:\n",
    "            try:\n",
    "                schema = yaml.safe_load(f)\n",
    "                if schema and 'classes' in schema:\n",
    "                    for class_name, class_def in schema['classes'].items():\n",
    "                        all_schemas[class_name] = class_def\n",
    "                        print(f\"  Found class: {class_name}\")\n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"Error parsing {schema_file}: {e}\")\n",
    "    \n",
    "    print(f\"Total classes loaded: {len(all_schemas)}\")\n",
    "    return all_schemas\n",
    "\n",
    "def get_full_schema(class_name, all_schemas, visited=None):\n",
    "    \"\"\"Recursively build complete schema including inheritance and mixins.\"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    # Prevent infinite recursion\n",
    "    if class_name in visited:\n",
    "        return {}\n",
    "    visited.add(class_name)\n",
    "    \n",
    "    if class_name not in all_schemas:\n",
    "        print(f\"Warning: Class '{class_name}' not found in schemas\")\n",
    "        return {}\n",
    "    \n",
    "    class_def = all_schemas.get(class_name, {})\n",
    "    if not class_def:\n",
    "        return {}\n",
    "        \n",
    "    # Start with this class's attributes\n",
    "    attributes = class_def.get('attributes', {}).copy()\n",
    "    \n",
    "    # Add parent class attributes (is_a relationship)\n",
    "    if 'is_a' in class_def:\n",
    "        parent_name = class_def['is_a']\n",
    "        print(f\"  {class_name} inherits from {parent_name}\")\n",
    "        parent_attributes = get_full_schema(parent_name, all_schemas, visited.copy())\n",
    "        # Parent attributes come first, then are overridden by child attributes\n",
    "        attributes = {**parent_attributes, **attributes}\n",
    "        \n",
    "    # Add mixin attributes\n",
    "    if 'mixins' in class_def:\n",
    "        for mixin in class_def['mixins']:\n",
    "            print(f\"  {class_name} uses mixin {mixin}\")\n",
    "            mixin_attributes = get_full_schema(mixin, all_schemas, visited.copy())\n",
    "            # Mixins come first, then are overridden by class attributes\n",
    "            attributes = {**mixin_attributes, **attributes}\n",
    "            \n",
    "    return attributes\n",
    "\n",
    "def detect_file_type(file_info):\n",
    "    \"\"\"Detect whether a file should use ClinicalFile or OmicFile schema.\"\"\"\n",
    "    name = file_info.get('name', '').lower()\n",
    "    path = file_info.get('path', '').lower()\n",
    "    \n",
    "    # Omic data indicators\n",
    "    omic_indicators = [\n",
    "        'metabolomics', 'proteomics', 'genomics', 'transcriptomics',\n",
    "        'somalogic', 'protavio', 'sequencing', 'omics',\n",
    "        '.adat', '.fastq', '.bam', '.vcf', '.bed'\n",
    "    ]\n",
    "    \n",
    "    # Clinical data indicators  \n",
    "    clinical_indicators = [\n",
    "        'clinical', 'assessment', 'medical', 'treatment', 'visit',\n",
    "        'demographic', 'alsfrs', 'vital', 'neurological', 'answer'\n",
    "    ]\n",
    "    \n",
    "    # Check for omic indicators\n",
    "    for indicator in omic_indicators:\n",
    "        if indicator in name or indicator in path:\n",
    "            return 'OmicFile'\n",
    "    \n",
    "    # Check for clinical indicators\n",
    "    for indicator in clinical_indicators:\n",
    "        if indicator in name or indicator in path:\n",
    "            return 'ClinicalFile'\n",
    "    \n",
    "    # Default to ClinicalFile for unknown types\n",
    "    return 'ClinicalFile'\n",
    "\n",
    "# Load all schemas from the data model\n",
    "print(\"ğŸ” LOADING DATA MODEL SCHEMAS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "all_schemas = get_all_schemas()\n",
    "\n",
    "if not all_schemas:\n",
    "    print(\"âŒ No schemas found! Check the path to model_schemas directory.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Loaded {len(all_schemas)} schema classes\")\n",
    "    \n",
    "    # Test schema loading for key classes\n",
    "    for class_name in ['ClinicalFile', 'OmicFile', 'BaseFile']:\n",
    "        if class_name in all_schemas:\n",
    "            print(f\"âœ… Found {class_name}\")\n",
    "        else:\n",
    "            print(f\"âŒ Missing {class_name}\")\n",
    "    \n",
    "    # Test schema inheritance\n",
    "    clinical_schema = get_full_schema('ClinicalFile', all_schemas)\n",
    "    omic_schema = get_full_schema('OmicFile', all_schemas)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Schema Analysis:\")\n",
    "    print(f\"   ClinicalFile attributes: {len(clinical_schema)}\")\n",
    "    print(f\"   OmicFile attributes: {len(omic_schema)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "annotation_management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸  ANNOTATION STRUCTURE GENERATION\n",
      "===================================\n",
      "ğŸ“‚ Existing annotations: 23 entries\n",
      "âœ… Existing annotations kept for: Answer Clinical Data\n",
      "âœ… Existing annotations kept for: 2024.T.15_Answer_ClinicalData_External.xlsx\n",
      "âœ… Existing annotations kept for: ICON\n",
      "âœ… Existing annotations kept for: 0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx\n",
      "âœ… Existing annotations kept for: 0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx\n",
      "âœ… Existing annotations kept for: 0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx\n",
      "âœ… Existing annotations kept for: Total File_0869-0004-A.xlsx\n",
      "âœ… Existing annotations kept for: Metabolomics\n",
      "âœ… Existing annotations kept for: 25_0805_Trehalose_EAP_C18-neg.xlsx\n",
      "âœ… Existing annotations kept for: 25_0805_Trehalose_EAP_C8-pos.xlsx\n",
      "âœ… Existing annotations kept for: 25_0805_Trehalose_EAP_HILIC-neg.xlsx\n",
      "âœ… Existing annotations kept for: 25_0805_Trehalose_EAP_HILIC-pos.xlsx\n",
      "âœ… Existing annotations kept for: Protavio\n",
      "âœ… Existing annotations kept for: PR-147-TES_RESULTS REPORT - Extended NPX.csv\n",
      "âœ… Existing annotations kept for: PR-147-TES_STUDY REPORT_v1.0.pdf\n",
      "âœ… Existing annotations kept for: PR-147-TES_STUDY REPORT_v2.0.pdf\n",
      "âœ… Existing annotations kept for: correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx\n",
      "âœ… Existing annotations kept for: Somalogic\n",
      "âœ… Existing annotations kept for: SS-25103982_SQS.pdf\n",
      "âœ… Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat\n",
      "âœ… Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat\n",
      "âœ… Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx\n",
      "âœ… Existing annotations kept for: SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx\n",
      "\n",
      "ğŸ“Š Merge Summary:\n",
      "   â• New entries: 0\n",
      "   âœ… Existing entries: 23\n",
      "   ğŸ“‹ Total entries: 23\n",
      "\n",
      "ğŸ’¾ Annotations saved to: ./annotations/trehalose_biomarker_data_annotations.json\n",
      "\n",
      "ğŸ“‹ Sample annotation structure for 'Answer Clinical Data':\n",
      "   clinicalDomain: ['subject_management', 'disease_progression']\n",
      "   studyPhase: longitudinal\n",
      "   visitType: \n",
      "   analysisTypes: ['']\n",
      "   annotations: ['']\n",
      "   title: \n",
      "   description: \n",
      "   alternateName: \n",
      "   creator: ['']\n",
      "   contributor: ['']\n",
      "   ... and 24 more attributes\n",
      "\n",
      "ğŸ¯ File type detected: ClinicalFile\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: DYNAMIC ANNOTATION STRUCTURE GENERATION\n",
    "\n",
    "def create_annotation_template(file_info, all_schemas):\n",
    "    \"\"\"Create blank annotation template based on file type detection.\"\"\"\n",
    "    file_type = detect_file_type(file_info)\n",
    "    \n",
    "    # Choose appropriate schema\n",
    "    if file_type == 'OmicFile':\n",
    "        schema_attributes = get_full_schema('OmicFile', all_schemas)\n",
    "    else:\n",
    "        schema_attributes = get_full_schema('ClinicalFile', all_schemas)\n",
    "    \n",
    "    # Build template from schema\n",
    "    template = {}\n",
    "    for attr_name, attr_def in schema_attributes.items():\n",
    "        # Handle multivalued attributes\n",
    "        if isinstance(attr_def, dict) and attr_def.get('multivalued', False):\n",
    "            template[attr_name] = ['']\n",
    "        else:\n",
    "            template[attr_name] = ''\n",
    "    \n",
    "    # Add metadata about detected file type\n",
    "    template['_file_type'] = file_type\n",
    "    template['_schema_source'] = 'data-model'\n",
    "    template['_created_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    return template\n",
    "\n",
    "def merge_annotations_smartly(existing_annotations, new_files_folders, all_schemas):\n",
    "    \"\"\"\n",
    "    Smart merge that:\n",
    "    1. Keeps existing annotations intact\n",
    "    2. Adds templates for new files/folders not in existing annotations\n",
    "    3. Does not overwrite any existing values\n",
    "    \"\"\"\n",
    "    merged = existing_annotations.copy()\n",
    "    new_count = 0\n",
    "    existing_count = 0\n",
    "    \n",
    "    for syn_id, file_info in new_files_folders.items():\n",
    "        if syn_id not in merged:\n",
    "            # New file/folder - create template\n",
    "            template = create_annotation_template(file_info, all_schemas)\n",
    "            merged[syn_id] = {\n",
    "                file_info['name']: template\n",
    "            }\n",
    "            print(f\"â• Added template for: {file_info['name']} (detected as {template['_file_type']})\")\n",
    "            new_count += 1\n",
    "        else:\n",
    "            print(f\"âœ… Existing annotations kept for: {file_info['name']}\")\n",
    "            existing_count += 1\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Merge Summary:\")\n",
    "    print(f\"   â• New entries: {new_count}\")\n",
    "    print(f\"   âœ… Existing entries: {existing_count}\")\n",
    "    print(f\"   ğŸ“‹ Total entries: {len(merged)}\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Execute annotation creation/merging\n",
    "print(\"ğŸ—ï¸  ANNOTATION STRUCTURE GENERATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if all_schemas and files_folders:\n",
    "    # Load existing annotations\n",
    "    existing_annotations = load_existing_annotations(annotation_file_path)\n",
    "    print(f\"ğŸ“‚ Existing annotations: {len(existing_annotations)} entries\")\n",
    "    \n",
    "    # Smart merge with new files/folders\n",
    "    updated_annotations = merge_annotations_smartly(existing_annotations, files_folders, all_schemas)\n",
    "    \n",
    "    # Save updated annotations\n",
    "    save_annotations(updated_annotations, annotation_file_path)\n",
    "    print(f\"\\nğŸ’¾ Annotations saved to: {annotation_file_path}\")\n",
    "    \n",
    "    # Show sample annotation structure\n",
    "    if updated_annotations:\n",
    "        first_entry_id = list(updated_annotations.keys())[0]\n",
    "        first_entry = updated_annotations[first_entry_id]\n",
    "        first_file_name = list(first_entry.keys())[0]\n",
    "        sample_annotation = first_entry[first_file_name]\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Sample annotation structure for '{first_file_name}':\")\n",
    "        # Show first 10 attributes\n",
    "        sample_attrs = list(sample_annotation.items())[:10]\n",
    "        for key, value in sample_attrs:\n",
    "            if isinstance(value, list) and len(value) > 3:\n",
    "                display_value = f\"{value[:3]}... (+{len(value)-3} more)\"\n",
    "            else:\n",
    "                display_value = str(value)\n",
    "            print(f\"   {key}: {display_value}\")\n",
    "        \n",
    "        if len(sample_annotation) > 10:\n",
    "            print(f\"   ... and {len(sample_annotation) - 10} more attributes\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ File type detected: {sample_annotation.get('_file_type', 'Unknown')}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  Cannot create annotations: missing schemas or files\")\n",
    "    updated_annotations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "validation_system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ANNOTATION VALIDATION\n",
      "=========================\n",
      "ğŸ§¹ Cleaning Preview:\n",
      "   ğŸ“Š Total fields: 731\n",
      "   âœ… Will keep: 63\n",
      "   ğŸ—‘ï¸ Will remove: 668\n",
      "   ğŸ¯ Preserved meaningful: 4\n",
      "ğŸ“„ Answer Clinical Data...\n",
      "   Original: 29 â†’ Cleaned: 5 (24 removed)\n",
      "   âœ… Keeping: clinicalDomain: ['subject_management', 'diseas\n",
      "   ğŸ—‘ï¸ Removing: visitType: \n",
      "ğŸ“„ 2024.T.15_Answer_ClinicalData_External.xlsx...\n",
      "   Original: 29 â†’ Cleaned: 5 (24 removed)\n",
      "   âœ… Keeping: clinicalDomain: ['subject_management', 'diseas\n",
      "   ğŸ—‘ï¸ Removing: visitType: \n",
      "ğŸ“„ ICON...\n",
      "   Original: 29 â†’ Cleaned: 6 (23 removed)\n",
      "   âœ… Keeping: clinicalDomain: ['biomarkers']\n",
      "   ğŸ—‘ï¸ Removing: visitType: \n",
      "ğŸ“‹ Loaded 23 annotation entries from: ./annotations/trehalose_biomarker_data_annotations.json\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ Answer Clinical Data: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ 2024.T.15_Answer_ClinicalData_External.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ ICON: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ 0869-0004-B_pT181_SAMPLE RESULTS_FINAL_09May2025.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ 0869-0004-C_Total Tau_SAMPLE RESULTS_FINAL_24Apr2025 (1).xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ 0869-0004-D_miR-206_SAMPLE RESULTS_FINAL_10JUL2025.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  ClinicalFile inherits from BaseFile\n",
      "  ClinicalFile uses mixin ClinicalFileMixin\n",
      "âŒ Total File_0869-0004-A.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ Metabolomics: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ 25_0805_Trehalose_EAP_C18-neg.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ 25_0805_Trehalose_EAP_C8-pos.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ 25_0805_Trehalose_EAP_HILIC-neg.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ 25_0805_Trehalose_EAP_HILIC-pos.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ Protavio: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ PR-147-TES_RESULTS REPORT - Extended NPX.csv: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ PR-147-TES_STUDY REPORT_v1.0.pdf: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ PR-147-TES_STUDY REPORT_v2.0.pdf: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ correlation_between_NPX_and_ALSFRSR_score_rates_of_change.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ Somalogic: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ SS-25103982_SQS.pdf: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.adat: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.adat: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.anmlSMP.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "  OmicFile inherits from BaseFile\n",
      "  OmicFile uses mixin OmicFileMixin\n",
      "âŒ SS-25103982_v5.0_Serum.bridge.hybNorm.medNormInt.plateScale.leakDetection.calibrate.anmlQC.qcCheck.xlsx: 5 validation errors\n",
      "   â€¢ Required field 'title' is empty\n",
      "   â€¢ Required field 'creator' is empty\n",
      "   â€¢ Required field 'keywords' is empty\n",
      "   ... and 2 more errors\n",
      "\n",
      "ğŸ“Š Validation Summary:\n",
      "   âœ… Valid: 0\n",
      "   âŒ Invalid: 23\n",
      "   ğŸ”¢ Total errors: 115\n",
      "   âš ï¸  Total warnings: 0\n",
      "\n",
      "ğŸ›‘ Please fix validation errors before proceeding to annotation application\n",
      "ğŸ“ Edit the annotation file: ./annotations/trehalose_biomarker_data_annotations.json\n",
      "ğŸ”„ Re-run this cell after making changes\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: LOAD AND VALIDATE ANNOTATIONS\n",
    "\n",
    "def validate_annotation_against_schema(annotation, file_type, all_schemas):\n",
    "    \"\"\"\n",
    "    Validate individual annotation against its schema.\n",
    "    Returns (is_valid, errors_list)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Get expected schema\n",
    "    if file_type == 'OmicFile':\n",
    "        expected_schema = get_full_schema('OmicFile', all_schemas)\n",
    "    else:\n",
    "        expected_schema = get_full_schema('ClinicalFile', all_schemas)\n",
    "    \n",
    "    # Check for required fields\n",
    "    for attr_name, attr_def in expected_schema.items():\n",
    "        if isinstance(attr_def, dict) and attr_def.get('required', False):\n",
    "            if attr_name not in annotation:\n",
    "                errors.append(f\"Missing required field: {attr_name}\")\n",
    "            elif not annotation[attr_name] or annotation[attr_name] == '' or annotation[attr_name] == ['']:\n",
    "                errors.append(f\"Required field '{attr_name}' is empty\")\n",
    "    \n",
    "    # Check multivalued field constraints\n",
    "    for attr_name, value in annotation.items():\n",
    "        if attr_name.startswith('_'):  # Skip metadata fields\n",
    "            continue\n",
    "            \n",
    "        if attr_name in expected_schema:\n",
    "            attr_def = expected_schema[attr_name]\n",
    "            if isinstance(attr_def, dict):\n",
    "                is_multivalued = attr_def.get('multivalued', False)\n",
    "                \n",
    "                if is_multivalued and not isinstance(value, list):\n",
    "                    errors.append(f\"Field '{attr_name}' should be a list (multivalued)\")\n",
    "                elif not is_multivalued and isinstance(value, list):\n",
    "                    warnings.append(f\"Field '{attr_name}' is a list but should be single value\")\n",
    "        else:\n",
    "            warnings.append(f\"Field '{attr_name}' not found in schema (may be deprecated)\")\n",
    "    \n",
    "    # Check for completely empty annotations\n",
    "    non_metadata_fields = {k: v for k, v in annotation.items() if not k.startswith('_')}\n",
    "    filled_fields = {\n",
    "        k: v for k, v in non_metadata_fields.items() \n",
    "        if v and v != '' and v != [''] and v != []\n",
    "    }\n",
    "    \n",
    "    if len(filled_fields) == 0:\n",
    "        warnings.append(\"No fields have been filled out yet\")\n",
    "    \n",
    "    return len(errors) == 0, errors, warnings\n",
    "\n",
    "def validate_all_annotations(annotations_data, all_schemas):\n",
    "    \"\"\"Validate all annotations and report issues.\"\"\"\n",
    "    validation_results = {}\n",
    "    total_errors = 0\n",
    "    total_warnings = 0\n",
    "    valid_count = 0\n",
    "    \n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            file_type = annotation.get('_file_type', 'ClinicalFile')\n",
    "            is_valid, errors, warnings = validate_annotation_against_schema(annotation, file_type, all_schemas)\n",
    "            \n",
    "            validation_results[syn_id] = {\n",
    "                'file_name': file_name,\n",
    "                'is_valid': is_valid,\n",
    "                'errors': errors,\n",
    "                'warnings': warnings,\n",
    "                'file_type': file_type\n",
    "            }\n",
    "            \n",
    "            total_errors += len(errors)\n",
    "            total_warnings += len(warnings)\n",
    "            \n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                if len(warnings) > 0:\n",
    "                    print(f\"âš ï¸  {file_name}: Valid with {len(warnings)} warnings\")\n",
    "                else:\n",
    "                    print(f\"âœ… {file_name}: Valid\")\n",
    "            else:\n",
    "                print(f\"âŒ {file_name}: {len(errors)} validation errors\")\n",
    "                for error in errors[:3]:  # Show first 3 errors\n",
    "                    print(f\"   â€¢ {error}\")\n",
    "                if len(errors) > 3:\n",
    "                    print(f\"   ... and {len(errors) - 3} more errors\")\n",
    "    \n",
    "    return validation_results, total_errors, total_warnings, valid_count\n",
    "\n",
    "\n",
    "def preview_annotation_cleaning(annotations_data, sample_limit=3):\n",
    "    \"\"\"Preview what the cleaning will do to annotations.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Import the cleaning function for testing\n",
    "    def is_meaningful_value(val):\n",
    "        preserved_values = {'Unknown', 'N/A', 'unknown', 'n/a', 'NA', 'na'}\n",
    "        if val in preserved_values:\n",
    "            return True\n",
    "        if isinstance(val, str):\n",
    "            return val.strip() != ''\n",
    "        return val is not None\n",
    "    \n",
    "    stats = {\n",
    "        'total_fields': 0,\n",
    "        'cleaned_fields': 0,\n",
    "        'removed_fields': 0,\n",
    "        'preserved_meaningful': 0\n",
    "    }\n",
    "    \n",
    "    examples = []\n",
    "    count = 0\n",
    "    \n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            original_count = 0\n",
    "            cleaned_count = 0\n",
    "            preserved_examples = []\n",
    "            removed_examples = []\n",
    "            \n",
    "            for key, value in annotation.items():\n",
    "                if not key.startswith('_'):\n",
    "                    original_count += 1\n",
    "                    stats['total_fields'] += 1\n",
    "                    \n",
    "                    # Simulate cleaning logic\n",
    "                    will_keep = False\n",
    "                    if isinstance(value, list):\n",
    "                        meaningful_items = [v for v in value if is_meaningful_value(v)]\n",
    "                        if meaningful_items:\n",
    "                            will_keep = True\n",
    "                            if any(item in {'Unknown', 'N/A'} for item in meaningful_items):\n",
    "                                stats['preserved_meaningful'] += 1\n",
    "                    elif is_meaningful_value(value):\n",
    "                        will_keep = True\n",
    "                        if value in {'Unknown', 'N/A'}:\n",
    "                            stats['preserved_meaningful'] += 1\n",
    "                    \n",
    "                    if will_keep:\n",
    "                        cleaned_count += 1\n",
    "                        stats['cleaned_fields'] += 1\n",
    "                        if len(preserved_examples) < 2:\n",
    "                            preserved_examples.append(f'{key}: {str(value)[:30]}')\n",
    "                    else:\n",
    "                        stats['removed_fields'] += 1\n",
    "                        if len(removed_examples) < 2:\n",
    "                            removed_examples.append(f'{key}: {str(value)[:30]}')\n",
    "            \n",
    "            if count < sample_limit:\n",
    "                examples.append({\n",
    "                    'file_name': file_name,\n",
    "                    'original': original_count,\n",
    "                    'cleaned': cleaned_count,\n",
    "                    'removed': original_count - cleaned_count,\n",
    "                    'preserved_examples': preserved_examples,\n",
    "                    'removed_examples': removed_examples\n",
    "                })\n",
    "                count += 1\n",
    "    \n",
    "    return stats, examples\n",
    "\n",
    "# Execute validation\n",
    "print(\"ğŸ” ANNOTATION VALIDATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "if os.path.exists(annotation_file_path):\n",
    "    # Load annotations for validation\n",
    "    annotations_data = load_existing_annotations(annotation_file_path)\n",
    "    \n",
    "    # Preview cleaning effects\n",
    "    cleaning_stats, cleaning_examples = preview_annotation_cleaning(annotations_data)\n",
    "    print(f\"ğŸ§¹ Cleaning Preview:\")\n",
    "    print(f\"   ğŸ“Š Total fields: {cleaning_stats['total_fields']}\")\n",
    "    print(f\"   âœ… Will keep: {cleaning_stats['cleaned_fields']}\")\n",
    "    print(f\"   ğŸ—‘ï¸ Will remove: {cleaning_stats['removed_fields']}\")\n",
    "    print(f\"   ğŸ¯ Preserved meaningful: {cleaning_stats['preserved_meaningful']}\")\n",
    "    \n",
    "    for example in cleaning_examples:\n",
    "        print(f\"ğŸ“„ {example['file_name'][:50]}...\")\n",
    "        print(f\"   Original: {example['original']} â†’ Cleaned: {example['cleaned']} ({example['removed']} removed)\")\n",
    "        if example['preserved_examples']:\n",
    "            print(f\"   âœ… Keeping: {example['preserved_examples'][0]}\")\n",
    "        if example['removed_examples']:\n",
    "            print(f\"   ğŸ—‘ï¸ Removing: {example['removed_examples'][0]}\")\n",
    "    print(f\"ğŸ“‹ Loaded {len(annotations_data)} annotation entries from: {annotation_file_path}\")\n",
    "    \n",
    "    if all_schemas and annotations_data:\n",
    "        # Run validation\n",
    "        validation_results, total_errors, total_warnings, valid_count = validate_all_annotations(annotations_data, all_schemas)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Validation Summary:\")\n",
    "        print(f\"   âœ… Valid: {valid_count}\")\n",
    "        print(f\"   âŒ Invalid: {len(validation_results) - valid_count}\")\n",
    "        print(f\"   ğŸ”¢ Total errors: {total_errors}\")\n",
    "        print(f\"   âš ï¸  Total warnings: {total_warnings}\")\n",
    "        \n",
    "        if total_errors > 0:\n",
    "            print(f\"\\nğŸ›‘ Please fix validation errors before proceeding to annotation application\")\n",
    "            print(f\"ğŸ“ Edit the annotation file: {annotation_file_path}\")\n",
    "            print(f\"ğŸ”„ Re-run this cell after making changes\")\n",
    "        else:\n",
    "            print(f\"\\nğŸš€ All annotations are valid! Ready to apply to Synapse entities\")\n",
    "            if total_warnings > 0:\n",
    "                print(f\"ğŸ“ Note: {total_warnings} warnings found (non-blocking)\")\n",
    "    else:\n",
    "        print(\"âŒ Cannot validate: missing schemas or annotation data\")\n",
    "        validation_results = {}\n",
    "        total_errors = 1  # Block progression\n",
    "else:\n",
    "    print(f\"âŒ Annotation file not found: {annotation_file_path}\")\n",
    "    print(\"ğŸ“ Please run the previous cells to create annotations first\")\n",
    "    validation_results = {}\n",
    "    total_errors = 1  # Block progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annotation_application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: APPLY ANNOTATIONS TO SYNAPSE ENTITIES\n",
    "\n",
    "def get_existing_synapse_annotations(syn, entity_id):\n",
    "    \"\"\"Get existing annotations from Synapse entity.\"\"\"\n",
    "    try:\n",
    "        entity = syn.get(entity_id, downloadFile=False)\n",
    "        return dict(entity.annotations) if hasattr(entity, 'annotations') and entity.annotations else {}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting annotations for {entity_id}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def clean_annotations_for_synapse(annotation):\n",
    "    \"\"\"Clean annotations by removing metadata fields and truly empty values.\n",
    "    \n",
    "    Preserves meaningful values like 'Unknown', 'N/A', but removes:\n",
    "    - Empty strings ('')\n",
    "    - Lists containing only empty strings (['''])\n",
    "    - Empty lists ([])\n",
    "    - None/null values\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    \n",
    "    # Values to preserve even if they might seem \"empty\"\n",
    "    preserved_values = {'Unknown', 'N/A', 'unknown', 'n/a', 'NA', 'na'}\n",
    "    \n",
    "    def is_meaningful_value(val):\n",
    "        \"\"\"Check if a value is meaningful (not truly empty).\"\"\"\n",
    "        if val in preserved_values:\n",
    "            return True\n",
    "        if isinstance(val, str):\n",
    "            return val.strip() != ''\n",
    "        return val is not None\n",
    "    \n",
    "    for key, value in annotation.items():\n",
    "        # Skip metadata fields (starting with underscore)\n",
    "        if key.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        # Handle list values\n",
    "        if isinstance(value, list):\n",
    "            # Keep only meaningful values in the list\n",
    "            cleaned_list = [v for v in value if is_meaningful_value(v)]\n",
    "            if cleaned_list:  # Only include non-empty lists\n",
    "                cleaned[key] = cleaned_list\n",
    "            # Skip completely empty lists\n",
    "        \n",
    "        # Handle single values\n",
    "        elif is_meaningful_value(value):\n",
    "            cleaned[key] = value\n",
    "        \n",
    "        # Skip truly empty values (None, '', etc.)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def apply_annotations_to_entity(syn, entity_id, new_annotations, dry_run=False):\n",
    "    \"\"\"Apply annotations to Synapse entity.\"\"\"\n",
    "    try:\n",
    "        if dry_run:\n",
    "            print(f\"ğŸ” DRY RUN: Would apply {len(new_annotations)} annotations to {entity_id}\")\n",
    "            return True\n",
    "        \n",
    "        entity = syn.get(entity_id, downloadFile=False)\n",
    "        \n",
    "        # Clean annotations (remove metadata fields and empty values)\n",
    "        clean_annotations = clean_annotations_for_synapse(new_annotations)\n",
    "        \n",
    "        if not clean_annotations:\n",
    "            print(f\"âš ï¸  No valid annotations to apply (all fields empty)\")\n",
    "            return True  # Not an error, just nothing to do\n",
    "        \n",
    "        entity.annotations = clean_annotations\n",
    "        syn.store(entity, forceVersion=False)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to apply annotations to {entity_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "def apply_all_annotations(syn, annotations_data, validation_results, dry_run=False):\n",
    "    \"\"\"Apply annotations to all validated entities.\"\"\"\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for syn_id, file_data in annotations_data.items():\n",
    "        for file_name, annotation in file_data.items():\n",
    "            # Only apply if validation passed\n",
    "            validation_result = validation_results.get(syn_id, {})\n",
    "            if not validation_result.get('is_valid', False):\n",
    "                print(f\"â­ï¸  Skipping {file_name} (validation failed)\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"ğŸ”„ Applying annotations to {file_name} ({syn_id})\")\n",
    "            \n",
    "            # Show what will be applied\n",
    "            clean_annotations = clean_annotations_for_synapse(annotation)\n",
    "            print(f\"   ğŸ“‹ {len(clean_annotations)} non-empty fields to apply\")\n",
    "            \n",
    "            success = apply_annotations_to_entity(syn, syn_id, annotation, dry_run)\n",
    "            \n",
    "            if success:\n",
    "                success_count += 1\n",
    "                print(f\"   âœ… Success\")\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"   âŒ Failed\")\n",
    "    \n",
    "    return success_count, failed_count, skipped_count\n",
    "\n",
    "# Execute annotation application (only if validation passed)\n",
    "print(\"ğŸš€ ANNOTATION APPLICATION TO SYNAPSE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check if we have the required data from previous cells\n",
    "required_vars = ['syn', 'annotations_data', 'validation_results', 'total_errors']\n",
    "missing_vars = [var for var in required_vars if var not in locals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"âŒ Missing required data: {', '.join(missing_vars)}\")\n",
    "    print(\"ğŸ“ Please run all previous cells first\")\n",
    "elif total_errors > 0:\n",
    "    print(f\"ğŸ›‘ Skipping annotation application due to {total_errors} validation errors\")\n",
    "    print(f\"ğŸ“ Please fix errors in: {annotation_file_path}\")\n",
    "    print(f\"ğŸ”„ Then re-run the validation cell and this cell\")\n",
    "elif not syn:\n",
    "    print(\"âŒ No Synapse connection available\")\n",
    "elif not annotations_data:\n",
    "    print(\"âŒ No annotation data available\")\n",
    "else:\n",
    "    print(f\"ğŸ” Ready to apply annotations to {len(annotations_data)} entities\")\n",
    "    print(f\"ğŸ“ Dry run mode: {DRY_RUN}\")\n",
    "    \n",
    "    # Apply annotations\n",
    "    success_count, failed_count, skipped_count = apply_all_annotations(\n",
    "        syn, annotations_data, validation_results, dry_run=DRY_RUN\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Application Results:\")\n",
    "    print(f\"   âœ… Success: {success_count}\")\n",
    "    print(f\"   âŒ Failed: {failed_count}\")\n",
    "    print(f\"   â­ï¸  Skipped: {skipped_count}\")\n",
    "    print(f\"   ğŸ“‹ Total processed: {success_count + failed_count + skipped_count}\")\n",
    "    \n",
    "    if DRY_RUN:\n",
    "        print(f\"\\nğŸ” This was a DRY RUN - no actual changes made\")\n",
    "        print(f\"ğŸ’¡ Set DRY_RUN = False in Cell 2 to apply changes\")\n",
    "    elif success_count > 0:\n",
    "        print(f\"\\nğŸ‰ Successfully applied annotations to {success_count} entities!\")\n",
    "        print(f\"ğŸ”— Check your entities in Synapse to see the applied annotations\")\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"\\nâš ï¸  {failed_count} entities failed to update - check error messages above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amp-als",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
